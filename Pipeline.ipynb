{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5389b104-5d05-4bcc-a18f-3d1307dc50ed",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Step 1: Read all the Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1896c4ba-f2f5-45b9-81b9-a398a3dbcafa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#use the python_files, file_read, util_multi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81d79b3-9f38-4082-ad6e-0f476c5f525a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Step 2: Setup LLM's and install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed91d40a-bf6c-4ca7-b214-b1ba868e63f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_if_undefined(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"Please provide your {var}\")\n",
    "\n",
    "_set_if_undefined(\"LANGCHAIN_API_KEY\")\n",
    "_set_if_undefined(\"TAVILY_API_KEY\")\n",
    "_set_if_undefined(\"OPENAI_API_KEY\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1efe1c15-0644-4b2b-9c78-0c67f5d3838e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "available_models = [\"mixtral-8x7b-instruct-v01\", \"llamaguard-7b\", \"gemma-7b-it\", \"mistral-7b-instruct-v02\", \"phi-2\",\n",
    "                    \"llama-2-70b-chat\", \"phi-3-mini-128k-instruct\", \"llama-3-8b-instruct\"]\n",
    "\n",
    "# Let's select the model from the available list\n",
    "model_selected = available_models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aeb29d72-02c5-40b8-a193-7b7982ccf25e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import httpx\n",
    "import os\n",
    "from dotenv import load_dotenv    \n",
    "load_dotenv()\n",
    "# As mentioned in the above Note try to setup Dell certificate in your environment to avoid verify=False(SSL verification is disabled).\n",
    "http_client=httpx.Client(verify=False)\n",
    "client = OpenAI(\n",
    "    base_url=os.environ[\"API_URL\"],\n",
    "    http_client=http_client,\n",
    "    api_key=os.environ[\"API_KEY\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e95e2d-e537-4f7c-a5d5-a2d1558b865b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## If you are using custom llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0aea2b6-c762-41cd-8234-e79695a8c6c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Iterator, List, Optional\n",
    "\n",
    "from pydantic import Field\n",
    "from langchain_core.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain_core.language_models.llms import LLM\n",
    "from langchain_core.outputs import GenerationChunk\n",
    "import httpx\n",
    "from openai import OpenAI\n",
    "\n",
    "class CustomOpenAILLM(LLM):\n",
    "    \"\"\"A custom LLM using OpenAI API for text generation.\n",
    "\n",
    "    This class integrates with OpenAI's API to provide text generation capabilities.\n",
    "    The number of characters from the prompt that the model echoes can be controlled\n",
    "    through the initialization parameters.\n",
    "\n",
    "    Example:\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            model = CustomOpenAILLM(api_url=\"https://api.openai.com/v1\", api_key=\"your_api_key\", model=\"text-davinci-003\")\n",
    "            result = model._call(\"Hello, how are you?\")\n",
    "            result = model._stream(\"Hello, how are you?\")\n",
    "    \"\"\"\n",
    "\n",
    "    api_url: str\n",
    "    \"\"\"The base URL for the OpenAI API.\"\"\"\n",
    "    \n",
    "    api_key: str\n",
    "    \"\"\"The API key for authenticating with OpenAI.\"\"\"\n",
    "    \n",
    "    model: str\n",
    "    \"\"\"The model name to use for the OpenAI API.\"\"\"\n",
    "\n",
    "    client: OpenAI = Field(None)\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    def __init__(self, **data):\n",
    "        super().__init__(**data)\n",
    "        http_client = httpx.Client(verify=False)\n",
    "        self.client = OpenAI(base_url=self.api_url, http_client=http_client, api_key=self.api_key)\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        \"\"\"Run the LLM on the given input.\n",
    "\n",
    "        Args:\n",
    "            prompt: The prompt to generate from.\n",
    "            stop: Stop words to use when generating. Model output is cut off at the\n",
    "                first occurrence of any of the stop substrings.\n",
    "            run_manager: Callback manager for the run.\n",
    "            **kwargs: Arbitrary additional keyword arguments.\n",
    "\n",
    "        Returns:\n",
    "            The model output as a string.\n",
    "        \"\"\"\n",
    "        if stop is not None:\n",
    "            raise ValueError(\"stop kwargs are not permitted.\")\n",
    "        \n",
    "        completion = self.client.completions.create(\n",
    "            model=self.model,\n",
    "            prompt=prompt,\n",
    "            max_tokens=1000,\n",
    "        )\n",
    "        output = completion.choices[0].text\n",
    "\n",
    "        # Debugging: Log the completion\n",
    "        if run_manager:\n",
    "            run_manager.on_llm_end(completion)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def _stream(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Iterator[GenerationChunk]:\n",
    "        \"\"\"Stream the LLM on the given prompt.\n",
    "\n",
    "        Args:\n",
    "            prompt: The prompt to generate from.\n",
    "            stop: Stop words to use when generating. Model output is cut off at the\n",
    "                first occurrence of any of these substrings.\n",
    "            run_manager: Callback manager for the run.\n",
    "            **kwargs: Arbitrary additional keyword arguments.\n",
    "\n",
    "        Returns:\n",
    "            An iterator of GenerationChunks.\n",
    "        \"\"\"\n",
    "        completion = self.client.completions.create(\n",
    "            model=self.model,\n",
    "            prompt=prompt,\n",
    "            max_tokens=1000,\n",
    "        )\n",
    "        for char in completion.choices[0].text:\n",
    "            chunk = GenerationChunk(text=char)\n",
    "            if run_manager:\n",
    "                run_manager.on_llm_new_token(chunk.text, chunk=chunk)\n",
    "            yield chunk\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return a dictionary of identifying parameters.\"\"\"\n",
    "        return {\n",
    "            \"model_name\": \"CustomOpenAILLM\",\n",
    "            \"api_url\": self.api_url,\n",
    "            \"model\": self.model,\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Get the type of language model used by this chat model.\"\"\"\n",
    "        return \"custom_openai\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac3c419-a10d-4c1f-a797-7d7af6977ba9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "custom_llm = CustomOpenAILLM(api_url=api_url, api_key=api_key, model=available_models[6])\n",
    "\n",
    "print(custom_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54668c9-577e-4149-be51-e63fa4343d66",
   "metadata": {
    "tags": []
   },
   "source": [
    "## In-built langchain-openAi llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b097bb-e9d1-4a85-a0a7-0a4e2e0539fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Importing Necessary Libraries for Conversational Buffer Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee729055-bbcb-448a-a0e3-48e257236236",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema import HumanMessage, AIMessage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b01f13-7a94-425a-babd-247732f8a24e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Simple Langchain ChatOpenAI for invoke functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "778dfb93-8a85-4bba-86ed-e725122a2386",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "langchain_llm = ChatOpenAI(base_url=os.environ['API_URL'],model = available_models[6], http_client=http_client, api_key=os.environ['API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36f0b327-8e42-4392-8dd0-eeae62b5dceb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "langchain_llm1 = ChatOpenAI(base_url=os.environ['API_URL'],model = available_models[3], http_client=http_client, api_key=os.environ['API_KEY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec428ade-bc41-4098-bb62-df07bda7af3d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Step 3: Some Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14997d9b-f5bd-44eb-8767-f36c1fa571fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Splitting into paths and contents for relevant information in a chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd98f21b-cf11-4506-873a-87b22b0c7e81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3192 3192\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def parse_output_file(filepath):\n",
    "    text =''\n",
    "    paths,contents = [],[]\n",
    "    encodings = ['utf-8', 'iso-8859-1', 'windows-1252', 'ascii']\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding=encoding) as file:\n",
    "                text =file.read()\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    matches = re.finditer(r'C:/.*',text)\n",
    "    matches1 = [match for match in matches]# make this the last section of the codebase\n",
    "    for match in matches1:\n",
    "        paths.append(match.group(0))\n",
    "    for match, match_next in zip(matches1[:-1],matches1[1:]):\n",
    "        ending_index = match.span()[1]\n",
    "        starting_index = match_next.span()[0]\n",
    "        contents.append(text[ending_index:starting_index])\n",
    "    \n",
    "    #appending the last file\n",
    "    contents.append(text[matches1[-1].span()[1]:])\n",
    "    return paths,contents\n",
    "\n",
    "paths,contents =parse_output_file(\"output2.txt\")\n",
    "print(len(paths), len(contents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b58ac00-e4f5-44ca-86b5-dccd496b443f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Split it into context-aware chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c1233b9-601a-4ab0-ab37-e2214d4b8bda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "def chunk_code(content: str, max_chunk_size: int = 2048) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split the code into context-aware chunks.\n",
    "    \"\"\"\n",
    "    # Split the content into lines\n",
    "    lines = content.split('\\n')\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_chunk_size = 0\n",
    "\n",
    "    for line in lines:\n",
    "        line_size = len(line) + 1  # +1 for the newline character\n",
    "        if current_chunk_size + line_size > max_chunk_size and current_chunk:\n",
    "            chunks.append('\\n'.join(current_chunk))\n",
    "            current_chunk = []\n",
    "            current_chunk_size = 0\n",
    "\n",
    "        current_chunk.append(line)\n",
    "        current_chunk_size += line_size\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append('\\n'.join(current_chunk))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7963049b-2b09-4ff1-8ba6-15694bb2b1f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3192\n"
     ]
    }
   ],
   "source": [
    "chunk_of_contents =[]\n",
    "for content in contents:\n",
    "    chunk_of_contents.append(chunk_code(content,10000))\n",
    "print(len(chunk_of_contents))\n",
    "#chunk of contents is a list[list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ee77f7-3d0a-4cea-ac32-c1e8b832089b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Step 4: Langgraph Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ba5fbef-8587-40d4-9407-c2d2a7fafd3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template1 = '''Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input you must take out only the sections of code that are useful for generation of test automation scripts\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought:{agent_scratchpad}'''\n",
    "\n",
    "prompt1 = PromptTemplate.from_template(template1)\n",
    "template2 = '''Generate the Testing scripts. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: Write PYTHON code and test it on the generate test automation scripts\n",
    "Thought:First check if you have enough information to perform the task\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer, If I can't generate a good enough final answer I will say,\"I dont have enough information\" or \"I need more information\"\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought:{agent_scratchpad}'''\n",
    "\n",
    "prompt2 = PromptTemplate.from_template(template2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92cbf4a2-ad17-4568-b5a2-2bb76c604365",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "# Create a ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(return_messages=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a229d45-0e62-4253-8ee5-d7cc6da1b346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m To answer the original question, I need to extract and summarize the functionality of the provided code, which appears to be part of an IPMI command interface in a C library. The code defines a set of functions related to IPMI operations such as getting device ID, self-test results, resetting/setting watchdog timers, reading/writing watchdog timers, setting BMC global enables, getting BMC global enables, clearing message flags, getting message flags, getting messages, sending messages, and retrieving the system UUID. Each function has a clear purpose, parameters, and a return type indicating the possibility of an unsupported operation (EFI_UNSUPPORTED).\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:\u001b[32;1m\u001b[1;3m To address the original question, let's break down the functionality of each function in the provided code. This library appears to be part of the IPMI command interface for a system that interacts with networked hardware via the Industry Standard Architecture (ISA) for Programmable Hardware (IPMI).\n",
      "\n",
      "**Key Functionalities:**\n",
      "\n",
      "1. **Device Management:**\n",
      "   - `EFIAPI IpmiGetDeviceId` retrieves the device ID.\n",
      "   - `EFIAPI IpmiGetSelfTestResult` gets the result of a self-test.\n",
      "\n",
      "2. **Watchdog Management:**\n",
      "   - `EFIAPI IpmiResetWatchdogTimer` resets the watchdog timer.\n",
      "   - `EFIAPI IpmiSetWatchdogTimer` sets the watchdog timer.\n",
      "   - `EFIAPI IpmiGetWatchdogTimer` gets the watchdog timer value.\n",
      "\n",
      "3. **BMC (Baseboard Management Controller) Control:**\n",
      "   - `EFIAPI IpmiSetBmcGlobalEnables` sets BMC global enables.\n",
      "   - `EFIAPI IpmiGetBmcGlobalEnables` retrieves BMC global enables.\n",
      "\n",
      "4. **Message Handling:**\n",
      "   - `EFIAPI IpmiClearMessageFlags` clears message flags.\n",
      "   - `EFIAPI IpmiGetMessageFlags` retrieves message flags.\n",
      "   - `EFIAPI IpmiGetMessage` gets a message.\n",
      "   - `EFIAPI IpmiSendMessage` sends a message.\n",
      "\n",
      "5. **System Information Retrieval:**\n",
      "   - `EFIAPI IpmiGetSystemUuid` retrieves the system's unique identifier (UUID).\n",
      "\n",
      "6. **Channel Data Access:**\n",
      "   - `EFIAPI IpmiGetChannelInfo` retrieves channel information.\n",
      "\n",
      "**Final Answer:**\n",
      "The provided code is a part of an IPMI command interface, allowing various operations related to device management, watchdog timer management, BMC control, message handling, system information retrieval, and channel data access. Each function is designed to interact with hardware components in a networked environment, adhering to a standardized protocol for programmable hardware management.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The summarized code snippets::  **\n",
      "The provided code is a part of an IPMI command interface, allowing various operations related to device management, watchdog timer management, BMC control, message handling, system information retrieval, and channel data access. Each function is designed to interact with hardware components in a networked environment, adhering to a standardized protocol for programmable hardware management.\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need more information to write the specific Python code and tests for this IPMI command interface. Here are some things I would need to know:\n",
      "\n",
      "1. The specific IPMI library or module being used in the code.\n",
      "2. The functions and their arguments in the IPMI code.\n",
      "3. The expected inputs and outputs for each function.\n",
      "4. Any specific edge cases or error handling that needs to be tested.\n",
      "\n",
      "Action: Search(\"IPMI Python library documentation\")\u001b[0mInvalid Format: Missing 'Action Input:' after 'Action:'\u001b[32;1m\u001b[1;3mCould not parse LLM output: ` Action Input: None (I will provide the input once I have the necessary documentation from the search)\n",
      "\n",
      "Action: Search(\"IPMI Python library documentation\")`\u001b[0mInvalid or incomplete response\u001b[32;1m\u001b[1;3m I need to provide a specific query for the search function to get accurate results. Here's an example of a more precise search query:\n",
      "\n",
      "Action: Search(\"IPMI Python library documentation - 'ipmi' or 'intelligent platform management interface'\")\u001b[0mInvalid Format: Missing 'Action Input:' after 'Action:'\u001b[32;1m\u001b[1;3mCould not parse LLM output: ` I will provide the search query as the input once I have it ready.\n",
      "\n",
      "Action Input: \"IPMI Python library documentation - 'ipmi' or 'intelligent platform management interface'\"\n",
      "\n",
      "Action: Search(query=\"IPMI Python library documentation - 'ipmi' or 'intelligent platform management interface'\")`\u001b[0mInvalid or incomplete response\u001b[32;1m\u001b[1;3mCould not parse LLM output: ` I need to provide a specific library name for the search query, such as \"Python-IPMI\" or \"ipmitool\" to get accurate results.\n",
      "\n",
      "Action Input: \"Python-IPMI or ipmitool IPMI Python library documentation\"\n",
      "\n",
      "Action: Search(query=\"Python-IPMI or ipmitool IPMI Python library documentation\")`\u001b[0mInvalid or incomplete response\u001b[32;1m\u001b[1;3mCould not parse LLM output: ` I need to install the Python-IPMI library to access its documentation and use it for testing. Here's the command to install it using pip:\n",
      "\n",
      "Action Input: \"pip install python-ipmi\"\n",
      "\n",
      "After installing the library, I'll try the search query again.\n",
      "\n",
      "Action: Search(query=\"Python-IPMI IPMI Python library documentation\")`\u001b[0mInvalid or incomplete response\u001b[32;1m\u001b[1;3mCould not parse LLM output: ` I need to provide a specific URL for the search query to get accurate results. Here's an example of a more precise search query:\n",
      "\n",
      "Action Input: \"Python-IPMI IPMI Python library documentation site:python.org\"\n",
      "\n",
      "Action: Search(query=\"Python-IPMI IPMI Python library documentation site:python.org\")`\u001b[0mInvalid or incomplete response\u001b[32;1m\u001b[1;3mCould not parse LLM output: ` I need to be more specific with my search query. Here's an updated search query:\n",
      "\n",
      "Action Input: \"Python-IPMI IPMI Python library documentation inurl:python.org\"\n",
      "\n",
      "Action: Search(query=\"Python-IPMI IPMI Python library documentation inurl:python.org\")`\u001b[0mInvalid or incomplete response\u001b[32;1m\u001b[1;3m I'm unable to provide a final answer as I don't have access to the specific IPMI library documentation or the exact functions and their arguments in the provided code. However, here's an outline of the steps you can take to write the Python code and tests for the IPMI command interface based on the given information:\n",
      "\n",
      "1. Install the Python-IPMI library using pip: `pip install python-ipmi`\n",
      "2. Search for the IPMI Python library documentation using a specific query: `Python-IPMI IPMI Python library documentation inurl:python.org`\n",
      "3. Review the documentation to understand the functions and their arguments, expected inputs, and outputs.\n",
      "4. Write Python code to test the functions using the provided code as a reference and the IPMI library documentation.\n",
      "5. Create test scripts to test each function with various inputs, including edge cases and error handling.\n",
      "\n",
      "Here's a sample test script for a function named `get_system_information()`:\n",
      "\n",
      "```python\n",
      "import unittest\n",
      "from python_ipmi import IpmiFactory, IpmiError\n",
      "\n",
      "class TestIPMI(unittest.TestCase):\n",
      "    def setUp(self):\n",
      "        self.ipmi = IpmiFactory.create_ipmi_snmp(\"127.0.0.1\")\n",
      "        self.ip = self.ipmi.get_ip()\n",
      "\n",
      "    def test_get_system_information(self):\n",
      "        try:\n",
      "            information = self.ipmi.get_system_information()\n",
      "            self.assertIsInstance(information, dict)\n",
      "            self.assertGreater(len(information), 0)\n",
      "            self.assertTrue(isinstance(information['chassis_id'], str))\n",
      "            self.assertTrue(isinstance(information['power_state'], int))\n",
      "        except IpmiError as error:\n",
      "            self.fail(f\"Failed to get system information: {error}\")\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    unittest.main()\n",
      "```\n",
      "\n",
      "This test script tests the `get_system_information()` function by creating an instance of the IPMI library and then attempting to retrieve system information. The test checks that the returned information is a dictionary, has a valid length, the chassis_id is a string, and the power_state is an integer.\n",
      "\n",
      "You can write similar test scripts for other functions in the IPMI code based on their expected inputs and outputs.\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:\u001b[32;1m\u001b[1;3m I apologize for the missing 'Action:' in the observation. Here's the corrected version:\n",
      "\u001b[0mInvalid Format: Missing 'Action Input:' after 'Action:'\u001b[32;1m\u001b[1;3mParsing LLM output produced both a final answer and a parse-able action::  I apologize for the errors in my previous response. Here is the corrected version:\n",
      "\n",
      "Question: **\n",
      "The provided code is a part of an IPMI command interface, allowing various operations related to device management, watchdog timer management, BMC control, message handling, system information retrieval, and channel data access. Each function is designed to interact with hardware components in a networked environment, adhering to a standardized protocol for programmable hardware management.\n",
      "\n",
      "Thought: I need more information to write the specific Python code and tests for this IPMI command interface. Here are some things I would need to know:\n",
      "\n",
      "1. The specific IPMI library or module being used in the code.\n",
      "2. The functions and their arguments in the IPMI code.\n",
      "3. The expected inputs and outputs for each function.\n",
      "4. Any specific edge cases or error handling that needs to be tested.\n",
      "\n",
      "Action: Search(\"IPMI Python library documentation - 'ipmi' or 'intelligent platform management interface'\")\n",
      "\n",
      "Thought: I need to provide a specific query for the search function to get accurate results. Here's an example of a more precise search query:\n",
      "\n",
      "Action Input: \"Python-IPMI or ipmitool IPMI Python library documentation\"\n",
      "\n",
      "Thought: I'm unable to provide a final answer as I don't have access to the specific IPMI library documentation or the exact functions and their arguments in the provided code. However, here's an outline of the steps you can take to write the Python code and tests for the IPMI command interface based on the given information:\n",
      "\n",
      "1. Install the Python-IPMI library using pip: `pip install python-ipmi`\n",
      "2. Search for the IPMI Python library documentation using a specific query: `Python-IPMI IPMI Python library documentation inurl:python.org`\n",
      "3. Review the documentation to understand the functions and their arguments, expected inputs, and outputs.\n",
      "4. Write Python code to test the functions using the provided code as a reference and the IPMI library documentation.\n",
      "5. Create test scripts to test each function with various inputs, including edge cases and error handling.\n",
      "\n",
      "Here's a sample test script for a function named `get_system_information()`:\n",
      "\n",
      "```python\n",
      "import unittest\n",
      "from python_ipmi import IpmiFactory, IpmiError\n",
      "\n",
      "class TestIPMI(unittest.TestCase):\n",
      "    def setUp(self):\n",
      "        self.ipmi = IpmiFactory.create_ipmi_snmp(\"127.0.0.1\")\n",
      "        self.ip = self.ipmi.get_ip()\n",
      "\n",
      "    def test_get_system_information(self):\n",
      "        try:\n",
      "            information = self.ipmi.get_system_information()\n",
      "            self.assertIsInstance(information, dict)\n",
      "            self.assertGreater(len(information), 0)\n",
      "            self.assertTrue(isinstance(information['chassis_id'], str))\n",
      "            self.assertTrue(isinstance(information['power_state'], int))\n",
      "        except IpmiError as error:\n",
      "            self.fail(f\"Failed to get system information: {error}\")\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    unittest.main()\n",
      "```\n",
      "\n",
      "This test script tests the `get_system_information()` function by creating an instance of the IPMI library and then attempting to retrieve system information. The test checks that the returned information is a dictionary, has a valid length, the chassis_id is a string, and the power_state is an integer.\n",
      "\n",
      "You can write similar test scripts for other functions in the IPMI code based on their expected inputs and outputs.\n",
      "\n",
      "Final Answer: To write the Python code and tests for the IPMI command interface, follow these steps:\n",
      "\n",
      "1. Install the Python-IPMI library using pip: `pip install python-ipmi`\n",
      "2. Search for the IPMI Python library documentation using a specific query: `Python-IPMI IPMI Python library documentation inurl:python.org`\n",
      "3. Review the documentation to understand the functions and their arguments, expected inputs, and outputs.\n",
      "4. Write Python code to test the functions using the provided code as a reference and the IPMI library documentation.\n",
      "5. Create test scripts to test each function with various inputs, including edge cases and error handling.\u001b[0mInvalid or incomplete response\u001b[32;1m\u001b[1;3m I apologize for the confusion in my previous response. Based on the given information, I cannot generate a complete Python code and test scripts for the IPMI command interface. However, I can provide you with an outline of the steps you should follow to write the code and tests.\n",
      "\n",
      "1. Install the Python-IPMI library using pip: `pip install python-ipmi`\n",
      "2. Search for the IPMI Python library documentation using a specific query, such as \"Python-IPMI IPMI Python library documentation inurl:python.org\" to find the official documentation on the Python.org website.\n",
      "3. Study the documentation to understand the functions, their arguments, expected inputs, and outputs.\n",
      "4. Write Python code to test each function using the provided code as a reference and the IPMI library documentation.\n",
      "5. Create test scripts using the unittest framework to test each function's behavior with various inputs, including edge cases and error handling.\n",
      "\n",
      "Here's a sample test script for a function named `get_system_information()`:\n",
      "\n",
      "```python\n",
      "import unittest\n",
      "from python_ipmi import IpmiFactory, IpmiError\n",
      "\n",
      "class TestIPMI(unittest.TestCase):\n",
      "    def setUp(self):\n",
      "        self.ipmi = IpmiFactory.create_ipmi_snmp(\"127.0.0.1\")\n",
      "        self.ip = self.ipmi.get_ip()\n",
      "\n",
      "    def test_get_system_information(self):\n",
      "        try:\n",
      "            result = self.ipmi.get_system_information()\n",
      "            self.assertIsInstance(result, dict)\n",
      "            self.assertTrue(isinstance(result['chassis_id'], str))\n",
      "            self.assertTrue(isinstance(result['power_state'], int))\n",
      "        except IpmiError as error:\n",
      "            self.fail(f\"Failed to get system information: {error}\")\n",
      "\n",
      "    def test_get_system_information_empty_response(self):\n",
      "        try:\n",
      "            result = self.ipmi.get_system_information()\n",
      "            self.assertIsNone(result)\n",
      "        except IpmiError as error:\n",
      "            self.fail(f\"Unexpected error: {error}\")\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    unittest.main()\n",
      "```\n",
      "\n",
      "This test script includes a test for the base case and an edge case where the function returns an empty response. You can write similar test scripts for other functions in the IPMI code based on their expected inputs and outputs.\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain_core.tools import Tool\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from typing import Annotated\n",
    "\n",
    "def get_current_time(*args,**kwargs):\n",
    "    \"\"\" returns date time in H:MM format\"\"\"\n",
    "    import datetime\n",
    "    now  = datetime.datetime.now()\n",
    "    return now.strftime(\"%I:%M %p\")\n",
    "\n",
    "\n",
    "repl = PythonREPL()\n",
    "def python_repl(\n",
    "    code: Annotated[str, \"The python code to execute to generate your chart.\"],\n",
    "):\n",
    "    \"\"\"Use this to execute python code. If you want to see the output of a value,\n",
    "    you should print it out with `print(...)`. This is visible to the user.\"\"\"\n",
    "    try:\n",
    "        result = repl.run(code)\n",
    "    except BaseException as e:\n",
    "        return f\"Failed to execute. Error: {repr(e)}\"\n",
    "    result_str = f\"Successfully executed:\\n```python\\n{code}\\n```\\nStdout: {result}\"\n",
    "    return (\n",
    "        result_str + \"\\n\\nIf you have completed all tasks, respond with FINAL ANSWER.\"\n",
    "    )\n",
    "tools = [\n",
    "    Tool(\n",
    "        name = 'Time',\n",
    "        func = get_current_time,\n",
    "        description = \"When you need to get the current time\"),\n",
    "    Tool(\n",
    "        name='pythonrepl',\n",
    "        func = python_repl,\n",
    "        description = \"To try and run your python code\",\n",
    "    ),\n",
    "    Tool(\n",
    "        name =\"Search\",\n",
    "        func = TavilySearchResults(max_results=1),\n",
    "        description = \"To search for information\"\n",
    "    )\n",
    "]\n",
    "\n",
    "llm = langchain_llm\n",
    "agent1 = create_react_agent(\n",
    "    llm = llm,\n",
    "    tools = tools,\n",
    "    prompt = prompt1,\n",
    "    stop_sequence = True\n",
    ")\n",
    "agent_executor1 = AgentExecutor.from_agent_and_tools(\n",
    "    agent = agent1,\n",
    "    tools = tools,\n",
    "    verbose = True,\n",
    "    handle_parsing_errors =True,\n",
    "    memory = memory\n",
    ")\n",
    "agent2 = create_react_agent(\n",
    "    llm = langchain_llm1,\n",
    "    tools = tools,\n",
    "    prompt = prompt2,\n",
    "    stop_sequence = True\n",
    ")\n",
    "agent_executor2 = AgentExecutor.from_agent_and_tools(\n",
    "    agent = agent2,\n",
    "    tools = tools,\n",
    "    verbose = True,\n",
    "    handle_parsing_errors =True,\n",
    "    memory = memory\n",
    ")\n",
    "# result = agent_executor.invoke({\"input\":\"Write code to make a graph of stock_prices over time\"})\n",
    "# print(result)\n",
    "def composed_chain(input_dict):\n",
    "    # Execute the first agent\n",
    "    result1 = agent_executor1.invoke({\n",
    "        \"input\": input_dict[\"input\"],\n",
    "    })\n",
    "    print(\"The summarized code snippets:: \", result1['output'])\n",
    "    print(\"------------------------------------------------------------------------\")\n",
    "    \n",
    "    # Execute the second agent, passing the result from the first\n",
    "    result2 = agent_executor2.invoke({\n",
    "        \"input\": result1[\"output\"],\n",
    "    })\n",
    "    \n",
    "    # Check if agent_executor2 has enough information\n",
    "    if \"I don't have enough information\" in result2[\"output\"] or \"I need more details\" in result2[\"output\"]:\n",
    "        return\n",
    "    else:\n",
    "        return result2\n",
    "\n",
    "# Wrap the function in a LangChain-compatible chain\n",
    "prompt=PromptTemplate(template=\"{input}\", input_variables=[\"input\"])\n",
    "for content in chunk_of_contents[600:]:\n",
    "    for chunk in content:\n",
    "        result = composed_chain({'input': chunk})\n",
    "        if result == None:\n",
    "            continue\n",
    "        else:\n",
    "            print(result)\n",
    "        \n",
    "        # Update memory\n",
    "        memory.save_context({\"input\": chunk}, {\"output\": result[\"output\"]})\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b35b33-df9a-4814-8108-2979c80c61c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f2292edd-a274-4a57-82cf-ad1f90c973a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "model= langchain_llm\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system',\"Given a chunk of code you identify and pick out only the parts that are important for an assistant to generate test automation scripts, do not give extra information\"),\n",
    "    ('human', \"tell me the parts of this {content} that are useful\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "daa12947-bfd7-4f50-a845-ccc0932c1dc7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " These constants are part of the OpenSSL project, specifically within the context of cryptographic operations related to RSA algorithms and their associated object identifiers (OIDs). Each constant represents a predefined OID that corresponds to a specific cryptographic algorithm or a set of algorithms. Here's a breakdown of what each constant represents:\n",
      "\n",
      "1. `ossl_der_oid_hashAlgs`: This OID corresponds to the hash algorithms that are considered \"well known\" and are precompiled in OpenSSL.\n",
      "\n",
      "2. `ossl_der_oid_rsaEncryption`: This OID is associated with the RSA encryption algorithm, as defined in PKCS #1 v1.5.\n",
      "\n",
      "3. `ossl_der_oid_id_RSAES-OAEP`: This OID is for the RSA Encryption with Optimal Asymmetric Encryption Padding (RSAES-OAEP), which is a more secure padding scheme than PKCS #1 v1.5.\n",
      "\n",
      "4. `ossl_der_oid_id_pSpecified`: This OID is associated with the \"pSpecified\" attribute in PKCS #1 v1.5, which allows the specification of an additional padding algorithm.\n",
      "\n",
      "5. `ossl_der_oid_id_RSASSA-PSS`: This OID stands for RSA Signature with Appendix Signature Algorithm - Probabilistic Signature Scheme (RSASSA-PSS), a modern signature scheme that provides cryptographic security.\n",
      "\n",
      "6. `ossl_der_oid_md2WithRSAEncryption`: This OID refers to an older MD2 hashing algorithm used in conjunction with RSA Encryption.\n",
      "\n",
      "7. `ossl_der_oid_md5WithRSAEncryption`: This OID corresponds to the MD5 hashing algorithm used in conjunction with RSA Encryption.\n",
      "\n",
      "8. `ossl_der_oid_sha1WithRSAEncryption`: This OID is for the SHA1 hashing algorithm used in conjunction with RSA Encryption.\n",
      "\n",
      "9. `ossl_der_oid_sha224WithRSAEncryption`: This OID corresponds to the SHA224 hashing algorithm used in conjunction with RSA Encryption.\n",
      "\n",
      "These constants are part of the code that generates DER (Distinguished Encoding Rules) encoded objects, which are used in cryptographic applications, particularly within OpenSSL. They help to standardize and ensure the proper use of cryptographic algorithms and their associated OIDs.\n"
     ]
    }
   ],
   "source": [
    "chain = prompt|model|StrOutputParser()\n",
    "result = chain.invoke({\"content\":chunk_of_contents[0][0]})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694b3d59-8ad7-4433-9498-0743847285ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
