{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"output2.txt\"\n",
    "output_dir = Path(f\"./output_dir/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vram\\CryptoPkg\\Test\\UnitTest\\Library\\BaseCryptLib\\TSTests.c \n",
      "/** @file\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def parse_output_file(filepath):\n",
    "    text =''\n",
    "    paths,contents = [],[]\n",
    "    with open(filepath, 'r',encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    matches = re.finditer(r'C:/.*',text)\n",
    "    matches1 = [match for match in matches]# make this the last section of the codebase\n",
    "    for match in matches1:\n",
    "        match = match.group(0)\n",
    "        match = match.split('/')[-1]\n",
    "        paths.append(match)\n",
    "    for match, match_next in zip(matches1[:-1],matches1[1:]):\n",
    "        ending_index = match.span()[1]\n",
    "        starting_index = match_next.span()[0]\n",
    "        contents.append(text[ending_index:starting_index])\n",
    "    return paths,contents\n",
    "    #appending the last file\n",
    "    # unique_dirs = set()\n",
    "    # contents.append(text[matches1[-1].span()[1]:])\n",
    "    # for path in paths:\n",
    "    #     path = path.replace('/','\\\\')\n",
    "    #     dir = path.split('\\\\')  \n",
    "    #     dir = dir[-heirarchy]\n",
    "    #     unique_dirs.add(dir)\n",
    "    \n",
    "    # dir_contents = {}\n",
    "    # for dir in unique_dirs:\n",
    "    #     dir_contents[dir] = []\n",
    "    # file_names = {}\n",
    "    # for path, content in zip(paths, contents):\n",
    "    #     path = path.replace('/', '\\\\')\n",
    "    #     dir = path.split('\\\\')[-heirarchy]\n",
    "    #     dir_contents[dir]+=str(content)\n",
    "    #     file_names[dir] = path.split('\\\\')[-1]\n",
    "    \n",
    "    # return file_names, dir_contents, unique_dirs\n",
    "paths,contents = parse_output_file(data_file)\n",
    "print(paths[0],contents[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13759\n",
      "/* resetin\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "class CustomTextLoader(TextLoader):\n",
    "    def __init__(self, file_path, **kwargs):\n",
    "        super().__init__(file_path, **kwargs)\n",
    "        self.file_path = file_path\n",
    "\n",
    "    def load(self):\n",
    "        paths, contents = parse_output_file(self.file_path)\n",
    "        documents = []\n",
    "        for path, content in zip(paths, contents):\n",
    "            metadata = {\"source\": path}\n",
    "            documents.append(Document(page_content=content.strip(), metadata=metadata))\n",
    "        return documents\n",
    "loader = CustomTextLoader(data_file, encoding='utf-8')\n",
    "documents = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=5000,\n",
    "    chunk_overlap = 100,\n",
    "    length_function = len,\n",
    "    is_separator_regex=False\n",
    ")\n",
    "pages = splitter.split_documents(documents)\n",
    "print(len(pages))\n",
    "pages = pages[10000:10100]  \n",
    "print(pages[3].page_content[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "api_key = os.getenv('API_KEY')\n",
    "base_url = os.getenv('API_URL')\n",
    "max_output_tokens = 1000\n",
    "streaming = False\n",
    "http_client = httpx.Client(verify=False)\n",
    "available_models = [\n",
    "    \"mixtral-8x7b-instruct-v01\", \n",
    "    \"gemma-7b-it\", \n",
    "    \"mistral-7b-instruct-v02\", \n",
    "    \"llama-2-70b-chat\", \n",
    "    \"phi-3-mini-128k-instruct\", \n",
    "    \"llama-3-8b-instruct\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "from yachalk import chalk\n",
    "from langchain_openai import ChatOpenAI,OpenAI\n",
    "llm = OpenAI(\n",
    "    base_url=base_url,\n",
    "    model=available_models[0],\n",
    "    http_client=http_client,\n",
    "    api_key=api_key,\n",
    "    max_tokens = 1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating all the utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Append the parent directory to the system path\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# Initialize the ChatOpenAI client\n",
    "\n",
    "def trim_incomplete_json(json_string):\n",
    "    # Find the last occurrence of '}]' or '},' in the string\n",
    "    last_complete = max(json_string.rfind('}]'), json_string.rfind('},'))\n",
    "    \n",
    "    if last_complete != -1:\n",
    "        # If found, trim the string to that point and add closing bracket if needed\n",
    "        trimmed = json_string[:last_complete+1]\n",
    "        if not trimmed.endswith(']'):\n",
    "            trimmed += ']'\n",
    "        return trimmed\n",
    "    else:\n",
    "        # If no complete object found, return empty list\n",
    "        return '[]'\n",
    "\n",
    "def extract_concepts(prompt: str, metadata: dict = {}) -> list:\n",
    "    SYS_PROMPT = (\n",
    "        \"Your task is to extract the key concepts (and non-personal entities) mentioned in the given context. \"\n",
    "        \"Extract only the most important and atomistic concepts, breaking them down into simpler concepts if needed. \"\n",
    "        \"Categorize the concepts into one of the following categories: \"\n",
    "        \"[import statement, concept, function definition, object-calling, document, class-definition, condition, misc].\\n\"\n",
    "        \"Format your output as a list of JSON objects in the following format:\\n\"\n",
    "        \"[\\n\"\n",
    "        \"   {\\n\"\n",
    "        '       \"entity\": \"The Concept\",\\n'\n",
    "        '       \"importance\": \"The contextual importance of the concept on a scale of 1 to 5 (5 being the highest)\",\\n'\n",
    "        '       \"category\": \"The Type of Concept\"\\n'\n",
    "        \"   },\\n\"\n",
    "        \"   {...}\\n\"\n",
    "        \"]\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYS_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    \n",
    "    response = llm.invoke(input=messages)\n",
    "    print(\"Extract Prompt \", response)\n",
    "    try:\n",
    "        result = json.loads(response)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"\\n\\nWARNING ### Incomplete JSON detected. Attempting to trim...\")\n",
    "        trimmed_response = trim_incomplete_json(response)\n",
    "        print(trimmed_response+\"\\n#####################################################################################################\")\n",
    "        try:\n",
    "            result = json.loads(trimmed_response)\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"\\n\\nERROR ### Failed to parse even after trimming. Here is the buggy response: \", response, \"\\n\\n\")\n",
    "            return None\n",
    "\n",
    "    if result is not None:\n",
    "        result = [dict(item, **metadata) for item in result]\n",
    "\n",
    "    return result\n",
    "\n",
    "def graph_prompt(input_text: str, metadata: dict = {}) -> list:\n",
    "    SYS_PROMPT = (\n",
    "        \"You are a network graph maker who extracts terms and their relations from a given context. \"\n",
    "        \"You are provided with a context chunk (delimited by ```). Your task is to extract the ontology \"\n",
    "        \"of terms mentioned in the given context. These terms should represent the key concepts according to the context.\\n\"\n",
    "        \"Thought 1: While traversing through each sentence, think about whether Data is being passed to it\\n\"\n",
    "        \"\\tTerms may include object creation, entity, class definition, import file, function signature, \\n\"\n",
    "        \"\\tcondition, parameters, documents, service, concept, etc.\\n\"\n",
    "        \"\\tTerms should be as concise as possible.\\n\\n\"\n",
    "        \"Thought 2: Think about how these terms can have one-on-one relations with other terms.\\n\"\n",
    "        \"\\tTerms mentioned in the same code or file are typically related to each other.\\n\"\n",
    "        \"\\tTerms can be related to many other terms.\\n\\n\"\n",
    "        \"Thought 3: Determine the relation between each related pair of terms.\\n\\n\"\n",
    "        \"Format your output as a list of JSON objects. Each element of the list contains a pair of terms do not provide an explanation, JUST THE JSON OUTPUT \"\n",
    "        \"and the relationship between them, as follows:\\n\"\n",
    "        \"[\\n\"\n",
    "        \"   {\\n\"\n",
    "        '       \"node_1\": \"A concept from the extracted ontology\",\\n'\n",
    "        '       \"node_2\": \"A related concept from the extracted ontology\",\\n'\n",
    "        '       \"edge\": \"The relationship between node_1 and node_2 in one or two sentences\"\\n'\n",
    "        \"   },\\n\"\n",
    "        \"   {...}\\n\"\n",
    "        \"]\"\n",
    "    )\n",
    "\n",
    "    USER_PROMPT = f\"context: ```{input_text}``` \\n\\n output: \"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYS_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": USER_PROMPT}\n",
    "    ]\n",
    "\n",
    "    response = llm.invoke(input=messages)\n",
    "    # print(\"Graph Prompt \", response)\n",
    "    try:\n",
    "        result = json.loads(response)\n",
    "    except json.JSONDecodeError:\n",
    "        # print(\"\\n\\nWARNING ### Incomplete JSON detected. Attempting to trim...\")\n",
    "        trimmed_response = trim_incomplete_json(response)\n",
    "        # print(trimmed_response)\n",
    "        # print(\"################################################################################################################\")\n",
    "        try:\n",
    "            result = json.loads(trimmed_response)\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"\\n\\nERROR ### Failed to parse even after trimming. Here is the buggy response: \")\n",
    "            return None\n",
    "\n",
    "    if result is not None:\n",
    "        result = [dict(item, **metadata) for item in result]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe and graph manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def documents2Dataframe(documents) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for chunk in documents:\n",
    "        row = {\n",
    "            \"text\": chunk.page_content,\n",
    "            \"path\": chunk.metadata[\"source\"],\n",
    "            **chunk.metadata,\n",
    "            \"chunk_id\": uuid.uuid4().hex,\n",
    "        }\n",
    "        rows = rows + [row]\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "\n",
    "\n",
    "def df2ConceptsList(dataframe: pd.DataFrame) -> list:\n",
    "    # dataframe.reset_index(inplace=True)\n",
    "    results = dataframe.apply(\n",
    "        lambda row: extract_concepts(\n",
    "            row.text, {\"chunk_id\": row.chunk_id, \"type\": \"concept\"}\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "    # invalid json results in NaN\n",
    "    results = results.dropna()\n",
    "    results = results.reset_index(drop=True)\n",
    "\n",
    "    ## Flatten the list of lists to one single list of entities.\n",
    "    concept_list = np.concatenate(results).ravel().tolist()\n",
    "    return concept_list\n",
    "\n",
    "\n",
    "def concepts2Df(concepts_list) -> pd.DataFrame:\n",
    "    ## Remove all NaN entities\n",
    "    concepts_dataframe = pd.DataFrame(concepts_list).replace(\" \", np.nan)\n",
    "    concepts_dataframe = concepts_dataframe.dropna(subset=[\"entity\"])\n",
    "    concepts_dataframe[\"entity\"] = concepts_dataframe[\"entity\"].apply(\n",
    "        lambda x: x.lower()\n",
    "    )\n",
    "\n",
    "    return concepts_dataframe\n",
    "\n",
    "\n",
    "def df2Graph(dataframe: pd.DataFrame, model=None) -> list:\n",
    "    total_rows = len(dataframe)\n",
    "    processed_rows = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    def process_row(row):\n",
    "        nonlocal processed_rows\n",
    "        result = graph_prompt(row.text, {\"chunk_id\": row.chunk_id,\"path\":row.path})\n",
    "        processed_rows += 1\n",
    "        elapsed_time = time.time() - start_time\n",
    "        avg_time_per_row = elapsed_time / processed_rows\n",
    "        estimated_time_remaining = (total_rows - processed_rows) * avg_time_per_row\n",
    "\n",
    "        print(f\"\\rProcessing: {processed_rows}/{total_rows} rows | \"\n",
    "              f\"Elapsed: {elapsed_time:.2f}s | \"\n",
    "              f\"Estimated time remaining: {estimated_time_remaining:.2f}s\", \n",
    "              end=\"\", flush=True)\n",
    "        return result\n",
    "\n",
    "    results = dataframe.apply(process_row, axis=1)\n",
    "\n",
    "    print(\"\\nProcessing complete!\")\n",
    "    # print(results)\n",
    "\n",
    "    # Filter out None values and flatten the list of lists to one single list of entities.\n",
    "    concept_list = [item for sublist in results if sublist is not None for item in sublist]\n",
    "    return concept_list\n",
    "\n",
    "def graph2Df(nodes_list) -> pd.DataFrame:\n",
    "    ## Remove all NaN entities\n",
    "    graph_dataframe = pd.DataFrame(nodes_list).replace(\" \", np.nan)\n",
    "    graph_dataframe = graph_dataframe.dropna(subset=[\"node_1\", \"node_2\"])\n",
    "    graph_dataframe[\"node_1\"] = graph_dataframe[\"node_1\"].apply(lambda x: x.lower())\n",
    "    graph_dataframe[\"node_2\"] = graph_dataframe[\"node_2\"].apply(lambda x: x.lower())\n",
    "\n",
    "    return graph_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = documents2Dataframe(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 16/100 rows | Elapsed: 189.94s | Estimated time remaining: 997.19ss\n",
      "\n",
      "ERROR ### Failed to parse even after trimming. Here is the buggy response: \n",
      "Processing: 33/100 rows | Elapsed: 389.74s | Estimated time remaining: 791.30s\n",
      "\n",
      "ERROR ### Failed to parse even after trimming. Here is the buggy response: \n",
      "Processing: 68/100 rows | Elapsed: 839.90s | Estimated time remaining: 395.25s\n",
      "\n",
      "ERROR ### Failed to parse even after trimming. Here is the buggy response: \n",
      "Processing: 100/100 rows | Elapsed: 1212.09s | Estimated time remaining: 0.00ss\n",
      "Processing complete!\n",
      "0     [{'node_1': 'libspdm_test_requester_encap_cert...\n",
      "1     [{'node_1': 'spdm_test_context', 'node_2': 'sp...\n",
      "2     [{'node_1': 'm_spdm_get_certificate_request3.l...\n",
      "3     [{'node_1': 'spdm_context', 'node_2': 'libspdm...\n",
      "4     [{'node_1': 'libspdm_test_context_t', 'node_2'...\n",
      "                            ...                        \n",
      "95    [{'node_1': 'libspdm_requester_encap_get_diges...\n",
      "96    [{'node_1': 'libspdm_test_context_t', 'node_2'...\n",
      "97    [{'node_1': 'spdm_test_context', 'node_2': 'sp...\n",
      "98    [{'node_1': 'spdm_test_context', 'node_2': 'sp...\n",
      "99    [{'node_1': 'libspdm_requester_encap_digests_t...\n",
      "Length: 100, dtype: object\n"
     ]
    }
   ],
   "source": [
    "## To regenerate the graph with LLM, set this to True\n",
    "regenerate = True\n",
    "\n",
    "if regenerate:\n",
    "    concepts_list = df2Graph(df)\n",
    "    dfg1 = graph2Df(concepts_list)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    dfg1.to_csv(output_dir/\"graph.csv\", sep=\"|\", index=False)\n",
    "    df.to_csv(output_dir/\"chunks.csv\", sep=\"|\", index=False)\n",
    "else:\n",
    "    dfg1 = pd.read_csv(output_dir/\"graph.csv\", sep=\"|\")\n",
    "\n",
    "dfg1.replace(\"\", np.nan, inplace=True)\n",
    "dfg1.dropna(subset=[\"node_1\", \"node_2\", 'edge'], inplace=True)\n",
    "dfg1['count'] = 4 \n",
    "## Increasing the weight of the relation to 4. \n",
    "## We will assign the weight of 1 when later the contextual proximity will be calculated.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## connecting node with more contextual proximity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adding count to the edges to design strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           chunk_id  \\\n",
      "0  bce693c03f8f47189cc4debbabb4aeb0   \n",
      "1  bce693c03f8f47189cc4debbabb4aeb0   \n",
      "2  dea3c0da87b64f3e90edcdd84b93e12e   \n",
      "3  dea3c0da87b64f3e90edcdd84b93e12e   \n",
      "4  dea3c0da87b64f3e90edcdd84b93e12e   \n",
      "\n",
      "                                                path variable  \\\n",
      "0  vram\\SecurityPkg\\DeviceSecurity\\SpdmLib\\libspd...   node_1   \n",
      "1  vram\\SecurityPkg\\DeviceSecurity\\SpdmLib\\libspd...   node_1   \n",
      "2  vram\\SecurityPkg\\DeviceSecurity\\SpdmLib\\libspd...   node_1   \n",
      "3  vram\\SecurityPkg\\DeviceSecurity\\SpdmLib\\libspd...   node_1   \n",
      "4  vram\\SecurityPkg\\DeviceSecurity\\SpdmLib\\libspd...   node_1   \n",
      "\n",
      "                                             node  \n",
      "0  libspdm_test_requester_encap_certificate_case1  \n",
      "1  libspdm_test_requester_encap_certificate_case1  \n",
      "2                               spdm_test_context  \n",
      "3                                    spdm_context  \n",
      "4                   spdm_context->connection_info  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node_1</th>\n",
       "      <th>node_2</th>\n",
       "      <th>edge</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>path</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>connection_info</td>\n",
       "      <td>version</td>\n",
       "      <td>connection_info contains version</td>\n",
       "      <td>7e313f371d84458091e17ab98b821fc8</td>\n",
       "      <td>vram\\SecurityPkg\\DeviceSecurity\\SpdmLib\\libspd...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>connection_info</td>\n",
       "      <td>connection_state</td>\n",
       "      <td>connection_info contains connection_state</td>\n",
       "      <td>7e313f371d84458091e17ab98b821fc8</td>\n",
       "      <td>vram\\SecurityPkg\\DeviceSecurity\\SpdmLib\\libspd...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>libspdm_requester_encap_digests_test_main</td>\n",
       "      <td>int</td>\n",
       "      <td>libspdm_requester_encap_digests_test_main is a...</td>\n",
       "      <td>892d8804db6a45d8980ba2a37aff41ae</td>\n",
       "      <td>vram\\SecurityPkg\\DeviceSecurity\\SpdmLib\\libspd...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>libspdm_requester_encap_digests_test_main</td>\n",
       "      <td>void</td>\n",
       "      <td>libspdm_requester_encap_digests_test_main is a...</td>\n",
       "      <td>892d8804db6a45d8980ba2a37aff41ae</td>\n",
       "      <td>vram\\SecurityPkg\\DeviceSecurity\\SpdmLib\\libspd...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>cmocka_unit_test</td>\n",
       "      <td>test_spdm_requester_encap_get_digests_case1</td>\n",
       "      <td>cmocka_unit_test is a macro that takes test_sp...</td>\n",
       "      <td>892d8804db6a45d8980ba2a37aff41ae</td>\n",
       "      <td>vram\\SecurityPkg\\DeviceSecurity\\SpdmLib\\libspd...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        node_1  \\\n",
       "327                            connection_info   \n",
       "328                            connection_info   \n",
       "329  libspdm_requester_encap_digests_test_main   \n",
       "330  libspdm_requester_encap_digests_test_main   \n",
       "331                           cmocka_unit_test   \n",
       "\n",
       "                                          node_2  \\\n",
       "327                                      version   \n",
       "328                             connection_state   \n",
       "329                                          int   \n",
       "330                                         void   \n",
       "331  test_spdm_requester_encap_get_digests_case1   \n",
       "\n",
       "                                                  edge  \\\n",
       "327                   connection_info contains version   \n",
       "328          connection_info contains connection_state   \n",
       "329  libspdm_requester_encap_digests_test_main is a...   \n",
       "330  libspdm_requester_encap_digests_test_main is a...   \n",
       "331  cmocka_unit_test is a macro that takes test_sp...   \n",
       "\n",
       "                             chunk_id  \\\n",
       "327  7e313f371d84458091e17ab98b821fc8   \n",
       "328  7e313f371d84458091e17ab98b821fc8   \n",
       "329  892d8804db6a45d8980ba2a37aff41ae   \n",
       "330  892d8804db6a45d8980ba2a37aff41ae   \n",
       "331  892d8804db6a45d8980ba2a37aff41ae   \n",
       "\n",
       "                                                  path  count  \n",
       "327  vram\\SecurityPkg\\DeviceSecurity\\SpdmLib\\libspd...      4  \n",
       "328  vram\\SecurityPkg\\DeviceSecurity\\SpdmLib\\libspd...      4  \n",
       "329  vram\\SecurityPkg\\DeviceSecurity\\SpdmLib\\libspd...      4  \n",
       "330  vram\\SecurityPkg\\DeviceSecurity\\SpdmLib\\libspd...      4  \n",
       "331  vram\\SecurityPkg\\DeviceSecurity\\SpdmLib\\libspd...      4  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def contextual_proximity(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Melt the dataframe into a list of nodes\n",
    "    dfg_long = pd.melt(\n",
    "        df, id_vars=[\"chunk_id\", \"path\"], value_vars=[\"node_1\", \"node_2\"], value_name=\"node\"\n",
    "    )\n",
    "    dfg_long.drop(columns=[\"variable\"], inplace=True)\n",
    "\n",
    "    # Self join with chunk id as the key will create a link between terms occurring in the same text chunk.\n",
    "    dfg_wide = pd.merge(dfg_long, dfg_long, on=[\"chunk_id\", \"path\"], suffixes=(\"_1\", \"_2\"))\n",
    "\n",
    "    # drop self loops\n",
    "    self_loops_drop = dfg_wide[dfg_wide[\"node_1\"] == dfg_wide[\"node_2\"]].index\n",
    "    dfg2 = dfg_wide.drop(index=self_loops_drop).reset_index(drop=True)\n",
    "\n",
    "    # Group and count edges.\n",
    "    dfg2 = (\n",
    "        dfg2.groupby([\"node_1\", \"node_2\"])\n",
    "        .agg({\"chunk_id\": [\",\".join, \"count\"], \"path\": \"first\"})\n",
    "        .reset_index()\n",
    "    )\n",
    "    dfg2.columns = [\"node_1\", \"node_2\", \"chunk_id\", \"count\", \"path\"]\n",
    "    dfg2.replace(\"\", np.nan, inplace=True)\n",
    "    dfg2.dropna(subset=[\"node_1\", \"node_2\"], inplace=True)\n",
    "    # Drop edges with 1 count\n",
    "    dfg2 = dfg2[dfg2[\"count\"] != 1]\n",
    "    dfg2[\"edge\"] = \"contextual proximity\"\n",
    "    return dfg2\n",
    "\n",
    "\n",
    "dfg2 = contextual_proximity(dfg1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node_1</th>\n",
       "      <th>node_2</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>edge</th>\n",
       "      <th>count</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>((libspdm_context_t*)spdm_context)-&gt;local_cont...</td>\n",
       "      <td>libspdm_read_responder_public_certificate_chain</td>\n",
       "      <td>41c0a30e7e5a4bd1ae7bd8cc63a2d818,41c0a30e7e5a4...</td>\n",
       "      <td>contextual proximity</td>\n",
       "      <td>5</td>\n",
       "      <td>vram\\SecurityPkg\\DeviceSecurity\\SpdmLib\\libspd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>**state</td>\n",
       "      <td>libspdm_return_t</td>\n",
       "      <td>d703f95925e341129f145f6f3d026d76,d703f95925e34...</td>\n",
       "      <td>contextual proximity</td>\n",
       "      <td>2</td>\n",
       "      <td>vram\\SecurityPkg\\DeviceSecurity\\SpdmLib\\libspd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>**state</td>\n",
       "      <td>libspdm_test_requester_encap_request_case4</td>\n",
       "      <td>d703f95925e341129f145f6f3d026d76,d703f95925e34...</td>\n",
       "      <td>contextual proximity</td>\n",
       "      <td>2</td>\n",
       "      <td>vram\\SecurityPkg\\DeviceSecurity\\SpdmLib\\libspd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>**state</td>\n",
       "      <td>void</td>\n",
       "      <td>d703f95925e341129f145f6f3d026d76,d703f95925e34...</td>\n",
       "      <td>contextual proximity</td>\n",
       "      <td>2</td>\n",
       "      <td>vram\\SecurityPkg\\DeviceSecurity\\SpdmLib\\libspd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>*state</td>\n",
       "      <td>libspdm_return_t</td>\n",
       "      <td>a3bac0cacfa9420c8271599e6d231d50,d703f95925e34...</td>\n",
       "      <td>contextual proximity</td>\n",
       "      <td>3</td>\n",
       "      <td>vram\\SecurityPkg\\DeviceSecurity\\SpdmLib\\libspd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>void</td>\n",
       "      <td>libspdm_test_context_t</td>\n",
       "      <td>d703f95925e341129f145f6f3d026d76,33cb62ae19e44...</td>\n",
       "      <td>contextual proximity</td>\n",
       "      <td>6</td>\n",
       "      <td>vram\\SecurityPkg\\DeviceSecurity\\SpdmLib\\libspd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>void</td>\n",
       "      <td>libspdm_test_requester_encap_request_case4</td>\n",
       "      <td>d703f95925e341129f145f6f3d026d76,d703f95925e34...</td>\n",
       "      <td>contextual proximity</td>\n",
       "      <td>4</td>\n",
       "      <td>vram\\SecurityPkg\\DeviceSecurity\\SpdmLib\\libspd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>void</td>\n",
       "      <td>spdm_dispatch</td>\n",
       "      <td>fa142ed632d7457bb252e6135c8dfcf0,fa142ed632d74...</td>\n",
       "      <td>contextual proximity</td>\n",
       "      <td>3</td>\n",
       "      <td>vram\\SecurityPkg\\DeviceSecurity\\SpdmLib\\libspd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>void</td>\n",
       "      <td>spdm_server_init</td>\n",
       "      <td>fa142ed632d7457bb252e6135c8dfcf0,fa142ed632d74...</td>\n",
       "      <td>contextual proximity</td>\n",
       "      <td>2</td>\n",
       "      <td>vram\\SecurityPkg\\DeviceSecurity\\SpdmLib\\libspd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708</th>\n",
       "      <td>void</td>\n",
       "      <td>status</td>\n",
       "      <td>d703f95925e341129f145f6f3d026d76,d703f95925e34...</td>\n",
       "      <td>contextual proximity</td>\n",
       "      <td>2</td>\n",
       "      <td>vram\\SecurityPkg\\DeviceSecurity\\SpdmLib\\libspd...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>709 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                node_1  \\\n",
       "0    ((libspdm_context_t*)spdm_context)->local_cont...   \n",
       "1                                              **state   \n",
       "2                                              **state   \n",
       "3                                              **state   \n",
       "4                                               *state   \n",
       "..                                                 ...   \n",
       "704                                               void   \n",
       "705                                               void   \n",
       "706                                               void   \n",
       "707                                               void   \n",
       "708                                               void   \n",
       "\n",
       "                                              node_2  \\\n",
       "0    libspdm_read_responder_public_certificate_chain   \n",
       "1                                   libspdm_return_t   \n",
       "2         libspdm_test_requester_encap_request_case4   \n",
       "3                                               void   \n",
       "4                                   libspdm_return_t   \n",
       "..                                               ...   \n",
       "704                           libspdm_test_context_t   \n",
       "705       libspdm_test_requester_encap_request_case4   \n",
       "706                                    spdm_dispatch   \n",
       "707                                 spdm_server_init   \n",
       "708                                           status   \n",
       "\n",
       "                                              chunk_id                  edge  \\\n",
       "0    41c0a30e7e5a4bd1ae7bd8cc63a2d818,41c0a30e7e5a4...  contextual proximity   \n",
       "1    d703f95925e341129f145f6f3d026d76,d703f95925e34...  contextual proximity   \n",
       "2    d703f95925e341129f145f6f3d026d76,d703f95925e34...  contextual proximity   \n",
       "3    d703f95925e341129f145f6f3d026d76,d703f95925e34...  contextual proximity   \n",
       "4    a3bac0cacfa9420c8271599e6d231d50,d703f95925e34...  contextual proximity   \n",
       "..                                                 ...                   ...   \n",
       "704  d703f95925e341129f145f6f3d026d76,33cb62ae19e44...  contextual proximity   \n",
       "705  d703f95925e341129f145f6f3d026d76,d703f95925e34...  contextual proximity   \n",
       "706  fa142ed632d7457bb252e6135c8dfcf0,fa142ed632d74...  contextual proximity   \n",
       "707  fa142ed632d7457bb252e6135c8dfcf0,fa142ed632d74...  contextual proximity   \n",
       "708  d703f95925e341129f145f6f3d026d76,d703f95925e34...  contextual proximity   \n",
       "\n",
       "     count                                               path  \n",
       "0        5  vram\\SecurityPkg\\DeviceSecurity\\SpdmLib\\libspd...  \n",
       "1        2  vram\\SecurityPkg\\DeviceSecurity\\SpdmLib\\libspd...  \n",
       "2        2  vram\\SecurityPkg\\DeviceSecurity\\SpdmLib\\libspd...  \n",
       "3        2  vram\\SecurityPkg\\DeviceSecurity\\SpdmLib\\libspd...  \n",
       "4        3  vram\\SecurityPkg\\DeviceSecurity\\SpdmLib\\libspd...  \n",
       "..     ...                                                ...  \n",
       "704      6  vram\\SecurityPkg\\DeviceSecurity\\SpdmLib\\libspd...  \n",
       "705      4  vram\\SecurityPkg\\DeviceSecurity\\SpdmLib\\libspd...  \n",
       "706      3  vram\\SecurityPkg\\DeviceSecurity\\SpdmLib\\libspd...  \n",
       "707      2  vram\\SecurityPkg\\DeviceSecurity\\SpdmLib\\libspd...  \n",
       "708      2  vram\\SecurityPkg\\DeviceSecurity\\SpdmLib\\libspd...  \n",
       "\n",
       "[709 rows x 6 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfg = pd.concat([dfg1, dfg2], axis=0)\n",
    "dfg = (\n",
    "    dfg.groupby([\"node_1\", \"node_2\"])\n",
    "    .agg({\n",
    "        \"chunk_id\": \",\".join,\n",
    "        \"edge\": \",\".join,\n",
    "        \"count\": \"sum\",\n",
    "        \"path\": lambda x: \",\".join(set(x))  # Combine unique paths\n",
    "    })\n",
    "    .reset_index()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(181,)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes = pd.concat([dfg['node_1'], dfg['node_2']], axis=0).unique()\n",
    "nodes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drawing the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "G = nx.Graph()\n",
    "\n",
    "## Add nodes to the graph\n",
    "for node in nodes:\n",
    "    G.add_node(\n",
    "        str(node)\n",
    "    )\n",
    "\n",
    "## Add edges to the graph\n",
    "for index, row in dfg.iterrows():\n",
    "    G.add_edge(\n",
    "        str(row[\"node_1\"]),\n",
    "        str(row[\"node_2\"]),\n",
    "        title=row[\"edge\"],\n",
    "        weight=row['count']/4\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Communities =  11\n",
      "[['((libspdm_context_t*)spdm_context)->local_context.local_cert_chain_provision_size[0]', '**state', '*state', '0', 'algorithm', 'base_asym_algo', 'base_hash_algo', 'buffer', 'buffer_size', 'capability', 'case_id', 'connection_info', 'connection_state', 'data', 'data_size', 'error_code', 'false', 'file_data', 'file_name', 'flags', 'libspdm_challenge', 'libspdm_context', 'libspdm_context_t', 'libspdm_context_t*', 'libspdm_context_t* spdm_context', 'libspdm_dump_hex_str', 'libspdm_get_asym_signature_size', 'libspdm_get_encap_response_certificate', 'libspdm_get_encap_response_key_update', 'libspdm_get_hash_size', 'libspdm_get_measurement', 'libspdm_hash_all', 'libspdm_read_input_file', 'libspdm_read_responder_public_certificate_chain', 'libspdm_read_responder_public_certificate_chain_by_size', 'libspdm_requester_challenge_test_receive_message', 'libspdm_requester_challenge_test_send_message', 'libspdm_requester_encap_get_digests_case1', 'libspdm_requester_encap_request_test_context', 'libspdm_requester_encap_request_test_receive_message', 'libspdm_requester_encap_request_test_send_message', 'libspdm_reset_message_b', 'libspdm_responder_data_sign', 'libspdm_return_t', 'libspdm_session_info_t', 'libspdm_session_info_t*', 'libspdm_set_standard_key_update_test_state', 'libspdm_status_invalid_msg_field', 'libspdm_status_send_fail', 'libspdm_status_success', 'libspdm_test_context', 'libspdm_test_context_t', 'libspdm_test_context_t*', 'libspdm_test_context_version', 'libspdm_test_requester_challenge_case10', 'libspdm_test_requester_challenge_case11', 'libspdm_test_requester_challenge_case12', 'libspdm_test_requester_challenge_case13', 'libspdm_test_requester_challenge_case18', 'libspdm_test_requester_challenge_case21', 'libspdm_test_requester_challenge_case26', 'libspdm_test_requester_challenge_case27', 'libspdm_test_requester_challenge_case28', 'libspdm_test_requester_challenge_case6', 'libspdm_test_requester_challenge_case7', 'libspdm_test_requester_encap_certificate_case1', 'libspdm_test_requester_encap_request_case3', 'libspdm_test_requester_encap_request_case4', 'libspdm_test_requester_encap_request_case8', 'libspdm_test_requester_encap_request_case9', 'libspdm_use_asym_algo', 'libspdm_use_hash_algo', 'local_cert_chain_provision_size', 'local_context', 'm_libspdm_local_certificate_chain', 'm_libspdm_use_asym_algo', 'm_libspdm_use_hash_algo', 'm_spdm_key_update_request1', 'm_spdm_key_update_request1_size', 'measurement_hash', 'measurement_record_length', 'null', 'peer_used_cert_chain', 'request_attribute', 'request_size', 'response_size', 'session_id', 'session_info', 'size_t', 'sizeof', 'sizeof(spdm_challenge_auth_response_t)', 'spdm_challenge_auth_response_t', 'spdm_connection_info_t', 'spdm_connection_state_t', 'spdm_context', 'spdm_context->connection_info', 'spdm_context->connection_info.capability', 'spdm_context->connection_info.capability.flags', 'spdm_context->connection_info.connection_state', 'spdm_context->connection_info.version', 'spdm_context->local_context', 'spdm_context->local_context.local_cert_chain_provision_size[0]', 'spdm_context->spdm_context', 'spdm_context_info', 'spdm_digest_response_t', 'spdm_dispatch', 'spdm_key_update_response_t', 'spdm_local_context_t', 'spdm_message_version_11', 'spdm_read_responder_public_certificate_chain', 'spdm_server_init', 'spdm_test_context', 'spdm_test_context->spdm_context', 'spdm_version_number_shift_bit_t', 'spdm_version_number_t', 'state', 'status', 'test', 'test_libspdm_requester_encap_challenge_auth_case1', 'transcript', 'uint32_t', 'uint8_t', 'version', 'void'], ['build_response_func', 'libspdm_requester_chunk_get_test_case1_build_certificates_response', 'libspdm_requester_chunk_get_test_case2_build_measurements_response'], ['cmocka_run_group_tests', 'libspdm_common_support_test_main', 'libspdm_unit_test_group_teardown', 'spdm_common_context_data_tests'], ['cmocka_unit_test', 'const struct cmunittest spdm_requester_challenge_tests[]', 'int', 'libspdm_requester_challenge_test_main', 'libspdm_requester_encap_digests_test_main', 'libspdm_test_requester_challenge_case1, libspdm_test_requester_challenge_case2, ...', 'test_spdm_requester_encap_get_digests_case1'], ['const struct cmunittest spdm_requester_encap_certificate_tests[]', 'libspdm_requester_encap_certificate_test_main', 'libspdm_setup_test_context', 'libspdm_unit_test_group_setup'], ['const struct cmunittest spdm_requester_key_update_tests', 'libspdm_requester_encap_key_update_test_main', 'test_libspdm_requester_encap_key_update_case1'], ['header', 'libspdm_encapsulated_request_response_t', 'libspdm_local_buffer', 'libspdm_test_transport_header_size', 'm_libspdm_local_buffer', 'm_libspdm_local_buffer_size', 'param1', 'param2', 'ptr', 'request_response_code', 'spdm_encapsulated_request_response_t', 'spdm_error_response_t', 'spdm_get_certificate', 'spdm_key_update_request_t', 'spdm_response', 'spdm_response_size', 'spdm_version', 'temp_buf', 'temp_buf_ptr', 'transport_header_size'], ['libspdm', 'libspdm_requester_lib.h', 'libspdm_secured_message_lib.h', 'spdm_unit_test.h', 'unit_test'], ['libspdm_max_cert_chain_block_len', 'm_spdm_get_certificate_request3.length'], ['libspdm_max_spdm_msg_size', 'libspdm_test_message_header_t', 'temp_buff', 'temp_buff_ptr', 'temp_buff_size'], ['libspdm_secured_message_context_t', 'm_req_secret_buffer', 'm_rsp_secret_buffer', 'secured_message_context']]\n"
     ]
    }
   ],
   "source": [
    "communities_generator = nx.community.girvan_newman(G)\n",
    "top_level_communities = next(communities_generator)\n",
    "next_level_communities = next(communities_generator)\n",
    "communities = sorted(map(sorted, next_level_communities))\n",
    "print(\"Number of Communities = \", len(communities))\n",
    "print(communities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "palette = \"hls\"\n",
    "\n",
    "## Now add these colors to communities and make another dataframe\n",
    "def colors2Community(communities) -> pd.DataFrame:\n",
    "    ## Define a color palette\n",
    "    p = sns.color_palette(palette, len(communities)).as_hex()\n",
    "    random.shuffle(p)\n",
    "    rows = []\n",
    "    group = 0\n",
    "    for community in communities:\n",
    "        color = p.pop()\n",
    "        group += 1\n",
    "        for node in community:\n",
    "            rows += [{\"node\": node, \"color\": color, \"group\": group}]\n",
    "    df_colors = pd.DataFrame(rows)\n",
    "    return df_colors\n",
    "\n",
    "\n",
    "colors = colors2Community(communities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in colors.iterrows():\n",
    "    G.nodes[row['node']]['group'] = row['group']\n",
    "    G.nodes[row['node']]['color'] = row['color']\n",
    "    G.nodes[row['node']]['size'] = G.degree[row['node']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./docs/index.html\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "\n",
    "\n",
    "k_value = 2  # Adjust this value to change node spacing\n",
    "pos = nx.spring_layout(G, k=k_value, iterations=50)\n",
    "\n",
    "\n",
    "net = Network(notebook=False, cdn_resources=\"remote\", height=\"1200px\", width=\"1200px\")\n",
    "\n",
    "\n",
    "for node, (x, y) in pos.items():\n",
    "    net.add_node(\n",
    "        node,\n",
    "        x=x * 200, \n",
    "        y=y * 200,\n",
    "        physics=True,  \n",
    "        **G.nodes[node] \n",
    "    )\n",
    "\n",
    "# Add edges to the Pyvis network\n",
    "for source, target, edge_attrs in G.edges(data=True):\n",
    "    edge_data = edge_attrs.copy()\n",
    "    edge_data['value'] = edge_data['weight']\n",
    "    net.add_edge(\n",
    "        source,\n",
    "        target,\n",
    "        **edge_data\n",
    "    )\n",
    "\n",
    "# Disable physics in Pyvis to maintain the NetworkX layout\n",
    "net.toggle_physics(True)\n",
    "\n",
    "# Save the network\n",
    "net.show(\"./docs/index.html\", notebook=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a Final Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_map = colors.set_index('node')['group'].to_dict()\n",
    "\n",
    "\n",
    "# Create new columns with default values\n",
    "dfg['community_node_1'] = dfg['node_1'].map(community_map)\n",
    "dfg['community_node_2'] = dfg['node_2'].map(community_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Community counts for 'community_node_1':\n",
      "community_node_1\n",
      "1     553\n",
      "7      90\n",
      "4      18\n",
      "5      10\n",
      "10      9\n",
      "8       8\n",
      "3       6\n",
      "11      6\n",
      "2       4\n",
      "6       4\n",
      "9       1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Community counts for 'community_node_2':\n",
      "community_node_2\n",
      "1     553\n",
      "7      90\n",
      "4      18\n",
      "5      10\n",
      "10      9\n",
      "8       8\n",
      "3       6\n",
      "11      6\n",
      "2       4\n",
      "6       4\n",
      "9       1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "community_count_node_1 = dfg['community_node_1'].value_counts(dropna=False)\n",
    "\n",
    "# Count the number of nodes in each community for 'community_node_2'\n",
    "community_count_node_2 = dfg['community_node_2'].value_counts(dropna=False)\n",
    "\n",
    "# Output the results\n",
    "# print(\"Community counts for 'community_node_1':\")\n",
    "# print(community_count_node_1)\n",
    "# print(\"\\nCommunity counts for 'community_node_2':\")\n",
    "# print(community_count_node_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg.to_csv('updated_graph.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating an Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reads one Line of the CSV and generates test script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_stretch = pd.read_csv('updated_graph.csv')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['node_1', 'node_2', 'chunk_id', 'edge', 'count', 'path',\n",
       "       'community_node_1', 'community_node_2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_stretch.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to first analyze the CSV data and get the first row, then generate a test automation script using the first row as input.\n",
      "Action: CSVAnalyzer\n",
      "Action Input: first row\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m{'node_1': '((libspdm_context_t*)spdm_context)->local_context.local_cert_chain_provision_size[0]', 'node_2': 'libspdm_read_responder_public_certificate_chain', 'chunk_id': '41c0a30e7e5a4bd1ae7bd8cc63a2d818,41c0a30e7e5a4bd1ae7bd8cc63a2d818,41c0a30e7e5a4bd1ae7bd8cc63a2d818,41c0a30e7e5a4bd1ae7bd8cc63a2d818,41c0a30e7e5a4bd1ae7bd8cc63a2d818', 'edge': 'contextual proximity', 'count': 5, 'path': 'vram\\\\SecurityPkg\\\\DeviceSecurity\\\\SpdmLib\\\\libspdm\\\\unit_test\\\\test_spdm_requester\\\\challenge.c', 'community_node_1': 1, 'community_node_2': 1}\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Now I have the first row of the CSV data, I can use this information to generate a test automation script.\n",
      "Action: TestGenerator\n",
      "Action Input: {'node_1': '((libspdm_context_t*)spdm_context)->local_context.local_cert_chain_provision_size[0]', 'node_2': 'libspdm_read_responder_public_certificate_chain', 'edge': 'contextual proximity', 'path': 'vram\\\\SecurityPkg\\\\DeviceSecurity\\\\SpdmLib\\\\libspdm\\\\unit_test\\\\test_spdm_requester\\\\challenge.c'}\u001b[0m#############################################\n",
      "<class 'dict'>\n",
      "#############################################\n",
      "generate_test_script called with node_1: ((libspdm_context_t*)spdm_context)->local_context.local_cert_chain_provision_size[0], node_2: libspdm_read_responder_public_certificate_chain, edge: contextual proximity, path: vram\\SecurityPkg\\DeviceSecurity\\SpdmLib\\libspdm\\unit_test\\test_spdm_requester\\challenge.c\n",
      "\n",
      "Observation: \u001b[38;5;200m\u001b[1;3m\n",
      "\n",
      "import os\n",
      "from spdm_requester_lib import SpdmRequesterLib\n",
      "from spdm_device_lib import SpdmDeviceLib\n",
      "from spdm_secured_message_lib import SpdmSecuredMessageLib\n",
      "from spdm_constant import SpdmConstant\n",
      "\n",
      "def read_file_contents(file_path):\n",
      "    with open(file_path, \"rb\") as file:\n",
      "        return file.read()\n",
      "\n",
      "def test_spdm_challenge():\n",
      "    # Set up test environment\n",
      "    requester = SpdmRequesterLib()\n",
      "    device = SpdmDeviceLib()\n",
      "\n",
      "    # Read file contents\n",
      "    cert_chain = read_file_contents(\"vram\\\\SecurityPkg\\\\DeviceSecurity\\\\SpdmLib\\\\libspdm\\\\unit_test\\\\test_spdm_requester\\\\challenge.c\")\n",
      "\n",
      "    # Simulate data flow between Node 1 and Node 2\n",
      "    requester.local_context.local_cert_chain_provision[0] = cert_chain\n",
      "\n",
      "    # Send a Challenge message to the device\n",
      "    secured_message_lib = SpdmSecuredMessageLib(requester, device)\n",
      "    response = secured_message_lib.send_receive_challenge_message()\n",
      "\n",
      "    # Verify edge condition\n",
      "    assert response is not None\n",
      "    assert requester.local_context.local_cert_chain_provision_size[0] > 0\n",
      "\n",
      "    # Clean up test environment\n",
      "    requester = None\n",
      "    device = None\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    test_spdm_challenge()\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I have the generated test automation script, I can now give the final answer.\n",
      "Final Answer: \n",
      "\n",
      "```python\n",
      "import os\n",
      "from spdm_requester_lib import SpdmRequesterLib\n",
      "from spdm_device_lib import SpdmDeviceLib\n",
      "from spdm_secured_message_lib import SpdmSecuredMessageLib\n",
      "from spdm_constant import SpdmConstant\n",
      "\n",
      "def read_file_contents(file_path):\n",
      "    with open(file_path, \"rb\") as file:\n",
      "        return file.read()\n",
      "\n",
      "def test_spdm_challenge():\n",
      "    # Set up test environment\n",
      "    requester = SpdmRequesterLib()\n",
      "    device = SpdmDeviceLib()\n",
      "\n",
      "    # Read file contents\n",
      "    cert_chain = read_file_contents(\"vram\\\\SecurityPkg\\\\DeviceSecurity\\\\SpdmLib\\\\libspdm\\\\unit_test\\\\test_spdm_requester\\\\challenge.c\")\n",
      "\n",
      "    # Simulate data flow between Node 1 and Node 2\n",
      "    requester.local_context.local_cert_chain_provision[0] = cert_chain\n",
      "\n",
      "    # Send a Challenge message to the device\n",
      "    secured_message_lib = SpdmSecuredMessageLib(requester, device)\n",
      "    response = secured_message_lib.send_receive_challenge_message()\n",
      "\n",
      "    # Verify edge condition\n",
      "    assert response is not None\n",
      "    assert requester.local_context.local_cert_chain_provision_size[0] > 0\n",
      "\n",
      "    # Clean up test environment\n",
      "    requester = None\n",
      "    device = None\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    test_spdm_challenge()\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "```python\n",
      "import os\n",
      "from spdm_requester_lib import SpdmRequesterLib\n",
      "from spdm_device_lib import SpdmDeviceLib\n",
      "from spdm_secured_message_lib import SpdmSecuredMessageLib\n",
      "from spdm_constant import SpdmConstant\n",
      "\n",
      "def read_file_contents(file_path):\n",
      "    with open(file_path, \"rb\") as file:\n",
      "        return file.read()\n",
      "\n",
      "def test_spdm_challenge():\n",
      "    # Set up test environment\n",
      "    requester = SpdmRequesterLib()\n",
      "    device = SpdmDeviceLib()\n",
      "\n",
      "    # Read file contents\n",
      "    cert_chain = read_file_contents(\"vram\\\\SecurityPkg\\\\DeviceSecurity\\\\SpdmLib\\\\libspdm\\\\unit_test\\\\test_spdm_requester\\\\challenge.c\")\n",
      "\n",
      "    # Simulate data flow between Node 1 and Node 2\n",
      "    requester.local_context.local_cert_chain_provision[0] = cert_chain\n",
      "\n",
      "    # Send a Challenge message to the device\n",
      "    secured_message_lib = SpdmSecuredMessageLib(requester, device)\n",
      "    response = secured_message_lib.send_receive_challenge_message()\n",
      "\n",
      "    # Verify edge condition\n",
      "    assert response is not None\n",
      "    assert requester.local_context.local_cert_chain_provision_size[0] > 0\n",
      "\n",
      "    # Clean up test environment\n",
      "    requester = None\n",
      "    device = None\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    test_spdm_challenge()\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent\n",
    "from langchain.agents.initialize import initialize_agent\n",
    "from langchain.tools import Tool\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('updated_graph.csv')\n",
    "\n",
    "# Custom tool to analyze CSV data\n",
    "def analyze_csv(query):\n",
    "    if \"first row\" in query:\n",
    "        return df.iloc[0].to_dict()\n",
    "    elif \"column names\" in query:\n",
    "        return list(df.columns)\n",
    "    elif \"row count\" in query:\n",
    "        return len(df)\n",
    "    else:\n",
    "        return \"Query not recognized. Please ask about 'first row', 'column names', or 'row count'.\"\n",
    "\n",
    "csv_analyzer_tool = Tool(\n",
    "    name=\"CSVAnalyzer\",\n",
    "    func=analyze_csv,\n",
    "    description=\"Analyzes the CSV data. You can ask about 'first row', 'column names', or 'row count'.\"\n",
    ")\n",
    "\n",
    "# Custom tool to read file contents\n",
    "def read_file_contents(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r') as file:\n",
    "            return file.read()\n",
    "    return f\"File not found: {file_path}\"\n",
    "\n",
    "file_reader_tool = Tool(\n",
    "    name=\"FileReader\",\n",
    "    func=read_file_contents,\n",
    "    description=\"Reads the contents of a file given its path\"\n",
    ")\n",
    "\n",
    "# Custom tool to generate test automation scripts\n",
    "def generate_test_script(inputs):\n",
    "    print(\"#############################################\")\n",
    "    try:\n",
    "        dictionary = ast.literal_eval(inputs)\n",
    "    except:\n",
    "        dictionary['node_1'] = 'node_1'\n",
    "        dictionary['node_2'] = 'node_2'\n",
    "        dictionary['edge'] = 'edge'\n",
    "        dictionary['path'] = 'path'\n",
    "    print(type(dictionary))\n",
    "    print(\"#############################################\")\n",
    "    node_1 = dictionary['node_1']\n",
    "    node_2 = dictionary['node_2']\n",
    "    edge = dictionary['edge']\n",
    "    path = dictionary['path']\n",
    "    \n",
    "    print(f\"generate_test_script called with node_1: {node_1}, node_2: {node_2}, edge: {edge}, path: {path}\")\n",
    "    \n",
    "    template = \"\"\"\n",
    "    Create a Python test automation script for the following scenario:\n",
    "    - Node 1: {node_1}\n",
    "    - Node 2: {node_2}\n",
    "    - Edge: {edge}\n",
    "    - Path: {path}\n",
    "\n",
    "    The script should:\n",
    "    1. Set up the test environment\n",
    "    2. Read the file contents from the given path\n",
    "    3. Simulate the data flow between Node 1 and Node 2\n",
    "    4. Verify the edge condition\n",
    "    5. Clean up the test environment\n",
    "\n",
    "    Please provide the complete Python script.\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"node_1\", \"node_2\", \"edge\", \"path\"],\n",
    "        template=template\n",
    "    )\n",
    "    return llm.invoke(prompt.format(node_1=node_1, node_2=node_2, edge=edge, path=path))\n",
    "\n",
    "test_generator_tool = Tool(\n",
    "    name=\"TestGenerator\",\n",
    "    func=generate_test_script,\n",
    "    description=\"Generates a test automation script given inputs in dictionary form with the keys 'node_1', 'node_2', 'edge', and 'path'\"\n",
    ")\n",
    "\n",
    "# Create the main agent\n",
    "tools = [csv_analyzer_tool, file_reader_tool, test_generator_tool]\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Example usage\n",
    "result = agent.run(\"Analyze the CSV data to get the first row, then generate a test automation script based on that information.\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reads a Section of the CSV and generates test script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing rows 0 to 9 (Section 1/71):\n",
      "Elapsed time: 0.00 seconds\n",
      "Estimated time left: 0.14 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-30 16:53:12,456 - INFO - HTTP Request: POST https://opensource-challenger-api.prdlvgpu1.aiaccel.dell.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "2024-07-30 16:53:16,004 - INFO - HTTP Request: POST https://opensource-challenger-api.prdlvgpu1.aiaccel.dell.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "2024-07-30 16:53:23,618 - INFO - HTTP Request: POST https://opensource-challenger-api.prdlvgpu1.aiaccel.dell.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "2024-07-30 16:53:27,643 - INFO - HTTP Request: POST https://opensource-challenger-api.prdlvgpu1.aiaccel.dell.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "2024-07-30 16:54:24,229 - INFO - HTTP Request: POST https://opensource-challenger-api.prdlvgpu1.aiaccel.dell.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "2024-07-30 16:54:54,255 - INFO - HTTP Request: POST https://opensource-challenger-api.prdlvgpu1.aiaccel.dell.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "2024-07-30 16:55:00,229 - INFO - HTTP Request: POST https://opensource-challenger-api.prdlvgpu1.aiaccel.dell.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "2024-07-30 16:55:40,694 - INFO - HTTP Request: POST https://opensource-challenger-api.prdlvgpu1.aiaccel.dell.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "2024-07-30 16:56:06,804 - INFO - HTTP Request: POST https://opensource-challenger-api.prdlvgpu1.aiaccel.dell.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "2024-07-30 16:56:13,715 - INFO - HTTP Request: POST https://opensource-challenger-api.prdlvgpu1.aiaccel.dell.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "2024-07-30 16:56:44,721 - INFO - HTTP Request: POST https://opensource-challenger-api.prdlvgpu1.aiaccel.dell.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "2024-07-30 16:56:49,401 - INFO - HTTP Request: POST https://opensource-challenger-api.prdlvgpu1.aiaccel.dell.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "2024-07-30 16:56:54,383 - INFO - HTTP Request: POST https://opensource-challenger-api.prdlvgpu1.aiaccel.dell.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "2024-07-30 16:57:32,863 - INFO - HTTP Request: POST https://opensource-challenger-api.prdlvgpu1.aiaccel.dell.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "2024-07-30 16:57:38,739 - INFO - HTTP Request: POST https://opensource-challenger-api.prdlvgpu1.aiaccel.dell.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "2024-07-30 16:57:48,133 - INFO - HTTP Request: POST https://opensource-challenger-api.prdlvgpu1.aiaccel.dell.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "2024-07-30 16:58:08,816 - INFO - HTTP Request: POST https://opensource-challenger-api.prdlvgpu1.aiaccel.dell.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "2024-07-30 16:58:13,182 - INFO - HTTP Request: POST https://opensource-challenger-api.prdlvgpu1.aiaccel.dell.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "2024-07-30 16:58:17,199 - INFO - HTTP Request: POST https://opensource-challenger-api.prdlvgpu1.aiaccel.dell.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "2024-07-30 16:58:53,548 - INFO - HTTP Request: POST https://opensource-challenger-api.prdlvgpu1.aiaccel.dell.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "2024-07-30 16:58:55,793 - INFO - HTTP Request: POST https://opensource-challenger-api.prdlvgpu1.aiaccel.dell.com/v1/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for this section: 389.05 seconds\n",
      "\n",
      "Processing rows 10 to 19 (Section 2/71):\n",
      "Elapsed time: 389.05 seconds\n",
      "Estimated time left: 13422.11 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-30 16:59:16,988 - INFO - HTTP Request: POST https://opensource-challenger-api.prdlvgpu1.aiaccel.dell.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "2024-07-30 16:59:21,622 - INFO - HTTP Request: POST https://opensource-challenger-api.prdlvgpu1.aiaccel.dell.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "2024-07-30 16:59:31,260 - INFO - HTTP Request: POST https://opensource-challenger-api.prdlvgpu1.aiaccel.dell.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "2024-07-30 16:59:43,649 - INFO - HTTP Request: POST https://opensource-challenger-api.prdlvgpu1.aiaccel.dell.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "2024-07-30 16:59:58,834 - INFO - HTTP Request: POST https://opensource-challenger-api.prdlvgpu1.aiaccel.dell.com/v1/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain.agents.initialize import initialize_agent\n",
    "from langchain.tools import Tool\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "import re\n",
    "\n",
    "# Load the CSV file\n",
    "try:\n",
    "    df = pd.read_csv('updated_graph.csv')\n",
    "except Exception as e:\n",
    "    print(f\"Error loading CSV file: {e}\")\n",
    "    df = pd.DataFrame()  # Create an empty DataFrame if file loading fails\n",
    "\n",
    "# Function to get a section of the CSV\n",
    "def get_csv_section(df, start_row, end_row):\n",
    "    try:\n",
    "        df_filtered = df.drop(columns=['chunk_id'])\n",
    "        return df_filtered.iloc[start_row:end_row].to_dict(orient='records')\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting CSV section: {e}\")\n",
    "        return []\n",
    "\n",
    "# Custom tool to read file contents\n",
    "def read_file_contents(file_path):\n",
    "    try:\n",
    "        file_path = os.path.join(os.getcwd()+'\\\\TestRepositories\\\\', file_path)\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, 'r') as file:\n",
    "                return file.read()[:10000]  # Truncate to 10000 characters\n",
    "        return f\"File not found: {file_path}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error reading file: {e}\"\n",
    "\n",
    "file_reader_tool = Tool(\n",
    "    name=\"FileReader\",\n",
    "    func=read_file_contents,\n",
    "    description=\"Reads the contents of a file given its path\"\n",
    ")\n",
    "def write_markdown_output(identified_nodes, test_scripts, output_file=\"output.md\"):\n",
    "    with open(output_file, \"w\") as f:\n",
    "        f.write(\"# LangChain Test Automation Results\\n\\n\")\n",
    "        \n",
    "        f.write(\"## Identified Nodes\\n\\n\")\n",
    "        for node in identified_nodes:\n",
    "            f.write(f\"- Node 1: {node['node_1']}\\n\")\n",
    "            f.write(f\"  Node 2: {node['node_2']}\\n\")\n",
    "            f.write(f\"  Edge: {node['edge']}\\n\")\n",
    "            f.write(f\"  Priority: {node['priority']}\\n\\n\")\n",
    "        \n",
    "        f.write(\"## Generated Test Scripts\\n\\n\")\n",
    "        for i, script in enumerate(test_scripts, 1):\n",
    "            f.write(f\"### Script {i}\\n\\n\")\n",
    "            f.write(\"```python\\n\")\n",
    "            f.write(script)\n",
    "            f.write(\"\\n```\\n\\n\")\n",
    "            f.write(\"---\\n\\n\")\n",
    "# Custom tool to generate test automation scripts\n",
    "def generate_test_script(inputs):\n",
    "    try:\n",
    "        dictionary = ast.literal_eval(inputs)\n",
    "        node_1 = dictionary['node_1']\n",
    "        node_2 = dictionary['node_2']\n",
    "        edge = dictionary['edge']\n",
    "        path = dictionary['path']\n",
    "        file_contents = dictionary.get('file_contents', 'File contents not provided')[:10000]  # Truncate to 10000 characters\n",
    "\n",
    "        template = \"\"\"\n",
    "        Create a Python test automation script for the following scenario:\n",
    "        - Node 1: {node_1}\n",
    "        - Node 2: {node_2}\n",
    "        - Edge: {edge}\n",
    "        - Path: {path}\n",
    "        - File Contents: {file_contents}\n",
    "\n",
    "        The script should:\n",
    "        1. Set up the test environment\n",
    "        2. Read the file contents from the given path\n",
    "        3. Effectively test out data-flow with respect to the file context\n",
    "        4. Verify the edge condition\n",
    "        5. Clean up the test environment\n",
    "\n",
    "        Please provide the complete Python script.\n",
    "        \"\"\"\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"node_1\", \"node_2\", \"edge\", \"path\", 'file_contents'],\n",
    "            template=template\n",
    "        )\n",
    "        return llm.invoke(prompt.format(node_1=node_1, node_2=node_2, edge=edge, path=path, file_contents=file_contents))[:10000]  # Truncate to 10000 characters\n",
    "    except Exception as e:\n",
    "        return f\"Error generating test script: {e}\"\n",
    "\n",
    "test_generator_tool = Tool(\n",
    "    name=\"TestGenerator\",\n",
    "    func=generate_test_script,\n",
    "    description=\"Generates a test automation script given inputs in dictionary form with the keys 'node_1', 'node_2', 'edge', and 'path'\"\n",
    ")\n",
    "\n",
    "# Create the main agent\n",
    "tools = [file_reader_tool, test_generator_tool]\n",
    "\n",
    "try:\n",
    "    agent = initialize_agent(\n",
    "        tools,\n",
    "        llm,\n",
    "        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "        verbose=False\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing agent: {e}\")\n",
    "    agent = None\n",
    "\n",
    "# Function to extract list of dictionaries from LLM output\n",
    "def extract_dicts_from_llm_output(output):\n",
    "    try:\n",
    "        # Find all dictionary-like structures in the output\n",
    "        dict_pattern = r'\\{[^{}]*\\}'\n",
    "        dict_strings = re.findall(dict_pattern, output)\n",
    "        \n",
    "        # Parse each dictionary-like string\n",
    "        dicts = []\n",
    "        for d_str in dict_strings:\n",
    "            try:\n",
    "                d = ast.literal_eval(d_str)\n",
    "                if isinstance(d, dict) and all(key in d for key in ['node_1', 'node_2', 'edge', 'priority', 'path']):\n",
    "                    dicts.append(d)\n",
    "            except:\n",
    "                pass  # Ignore any strings that can't be parsed as dictionaries\n",
    "        \n",
    "        return dicts\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting dictionaries from LLM output: {e}\")\n",
    "        return []\n",
    "\n",
    "# Process the CSV in sections\n",
    "section_size = 10\n",
    "total_rows = len(df)\n",
    "test_scripts = []\n",
    "identified_nodes = []\n",
    "\n",
    "# Time tracking variables\n",
    "start_time = time.time()\n",
    "total_sections = (total_rows + section_size - 1) // section_size\n",
    "\n",
    "for section, start_row in enumerate(range(0, total_rows, section_size), 1):\n",
    "    section_start_time = time.time()\n",
    "    try:\n",
    "        end_row = min(start_row + section_size, total_rows)\n",
    "        csv_section = get_csv_section(df, start_row, end_row)\n",
    "\n",
    "        # Calculate estimated time and elapsed time\n",
    "        elapsed_time = time.time() - start_time\n",
    "        estimated_total_time = (elapsed_time / section) * total_sections\n",
    "        estimated_time_left = estimated_total_time - elapsed_time\n",
    "\n",
    "        print(f\"\\nProcessing rows {start_row} to {end_row - 1} (Section {section}/{total_sections}):\")\n",
    "        print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "        print(f\"Estimated time left: {estimated_time_left:.2f} seconds\")\n",
    "\n",
    "        # Ask the LLM to identify relevant sections and generate test scripts\n",
    "        llm_context = f\"\"\"\n",
    "        Here is a section of the CSV file (rows {start_row} to {end_row - 1}):\n",
    "\n",
    "        {csv_section}\n",
    "\n",
    "        Select which values of nodes, edges, paths are most relevant for SYSTEM LEVEL test automation scripts.\n",
    "        Assign them high or low priority for testing.\n",
    "        Write them in dictionary format with keys: 'node_1','node_2','edge','priority','path'.\n",
    "        Provide your selections as a list of dictionaries.\n",
    "        \"\"\"[:10000]  # Truncate to 10000 characters\n",
    "\n",
    "        llm_plan = llm.invoke(llm_context)\n",
    "        # print(f\"LLM Plan for rows {start_row} to {end_row - 1}:\")\n",
    "        # print(llm_plan[:1000])  # Print only first 1000 characters of the plan\n",
    "\n",
    "        # Extract the list of dictionaries from the LLM output\n",
    "        plan_list = extract_dicts_from_llm_output(llm_plan)\n",
    "\n",
    "        # Generate test scripts based on the plan\n",
    "        for item in plan_list:\n",
    "            node_1 = item['node_1']\n",
    "            node_2 = item['node_2']\n",
    "            edge = item['edge']\n",
    "            path = item['path']\n",
    "            priority = item['priority']\n",
    "            \n",
    "            identified_nodes.append({\n",
    "                'node_1': node_1,\n",
    "                'node_2': node_2,\n",
    "                'edge': edge,\n",
    "                'priority': priority\n",
    "            })\n",
    "            \n",
    "            # Generate the test script\n",
    "            script_input = str({\n",
    "                'node_1': node_1,\n",
    "                'node_2': node_2,\n",
    "                'edge': edge,\n",
    "                'path': path\n",
    "            })[:10000]  # Truncate to 10000 characters\n",
    "            if agent:\n",
    "                result = agent.run(f\"Use the TestGenerator tool to create a test script for: {script_input}\")\n",
    "                test_scripts.append(result[:10000])  # Truncate to 10000 characters\n",
    "            else:\n",
    "                print(\"Agent not initialized. Skipping test script generation.\")\n",
    "\n",
    "        # Print time taken for this section\n",
    "        section_time = time.time() - section_start_time\n",
    "        print(f\"Time taken for this section: {section_time:.2f} seconds\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing rows {start_row} to {end_row - 1}: {e}\")\n",
    "\n",
    "# Write output to markdown file\n",
    "write_markdown_output(identified_nodes, test_scripts)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTotal processing time: {total_time:.2f} seconds\")\n",
    "print(\"Results have been written to output.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
