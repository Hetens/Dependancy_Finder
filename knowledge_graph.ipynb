{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path\n",
    "import random\n",
    "import time\n",
    "import logging\n",
    "from tenacity import retry, wait_exponential, stop_after_attempt\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"output2.txt\"\n",
    "output_dir = Path(f\"./output_dir/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def parse_output_file(filepath):\n",
    "    text =''\n",
    "    paths,contents = [],[]\n",
    "    with open(filepath, 'r',encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    matches = re.finditer(r'C:/.*',text)\n",
    "    matches1 = [match for match in matches]# make this the last section of the codebase\n",
    "    for match in matches1:\n",
    "        match = match.group(0)\n",
    "        match = match.split('/')[-1]\n",
    "        paths.append(match)\n",
    "    for match, match_next in zip(matches1[:-1],matches1[1:]):\n",
    "        ending_index = match.span()[1]\n",
    "        starting_index = match_next.span()[0]\n",
    "        contents.append(text[ending_index:starting_index])\n",
    "    return paths,contents\n",
    "    #appending the last file\n",
    "    # unique_dirs = set()\n",
    "    # contents.append(text[matches1[-1].span()[1]:])\n",
    "    # for path in paths:\n",
    "    #     path = path.replace('/','\\\\')\n",
    "    #     dir = path.split('\\\\')  \n",
    "    #     dir = dir[-heirarchy]\n",
    "    #     unique_dirs.add(dir)\n",
    "    \n",
    "    # dir_contents = {}\n",
    "    # for dir in unique_dirs:\n",
    "    #     dir_contents[dir] = []\n",
    "    # file_names = {}\n",
    "    # for path, content in zip(paths, contents):\n",
    "    #     path = path.replace('/', '\\\\')\n",
    "    #     dir = path.split('\\\\')[-heirarchy]\n",
    "    #     dir_contents[dir]+=str(content)\n",
    "    #     file_names[dir] = path.split('\\\\')[-1]\n",
    "    \n",
    "    # return file_names, dir_contents, unique_dirs\n",
    "paths,contents = parse_output_file(data_file)\n",
    "print(paths[0],contents[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "class CustomTextLoader(TextLoader):\n",
    "    def __init__(self, file_path, **kwargs):\n",
    "        super().__init__(file_path, **kwargs)\n",
    "        self.file_path = file_path\n",
    "\n",
    "    def load(self):\n",
    "        paths, contents = parse_output_file(self.file_path)\n",
    "        documents = []\n",
    "        for path, content in zip(paths, contents):\n",
    "            metadata = {\"source\": path}\n",
    "            documents.append(Document(page_content=content.strip(), metadata=metadata))\n",
    "        return documents\n",
    "loader = CustomTextLoader(data_file, encoding='utf-8')\n",
    "documents = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=5000,\n",
    "    chunk_overlap = 100,\n",
    "    length_function = len,\n",
    "    is_separator_regex=False\n",
    ")\n",
    "pages = splitter.split_documents(documents)\n",
    "print(len(pages))\n",
    "pages = pages[10000:10100]  \n",
    "print(pages[3].page_content[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import os\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"]  = str(os.getenv(\"GROQ_API_KEY\"))\n",
    "\n",
    "http_client = httpx.Client(verify=False)\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-70b-versatile\",\n",
    "    temperature=0.3,\n",
    "    http_client=http_client,\n",
    "    max_tokens=100,\n",
    "    # other params...\n",
    ")\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "from yachalk import chalk\n",
    "from langchain_openai import ChatOpenAI,OpenAI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating all the utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Append the parent directory to the system path\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# Initialize the ChatOpenAI client\n",
    "\n",
    "def trim_incomplete_json(json_string):\n",
    "    # Find the last occurrence of '}]' or '},' in the string\n",
    "    last_complete = max(json_string.rfind('}]'), json_string.rfind('},'))\n",
    "    \n",
    "    if last_complete != -1:\n",
    "        # If found, trim the string to that point and add closing bracket if needed\n",
    "        trimmed = json_string[:last_complete+1]\n",
    "        if not trimmed.endswith(']'):\n",
    "            trimmed += ']'\n",
    "        return trimmed\n",
    "    else:\n",
    "        # If no complete object found, return empty list\n",
    "        return '[]'\n",
    "\n",
    "def extract_concepts(prompt: str, metadata: dict = {}) -> list:\n",
    "    SYS_PROMPT = (\n",
    "        \"Your task is to extract the key concepts (and non-personal entities) mentioned in the given context. \"\n",
    "        \"Extract only the most important and atomistic concepts, breaking them down into simpler concepts if needed. \"\n",
    "        \"Categorize the concepts into one of the following categories: \"\n",
    "        \"[import statement, concept, function definition, object-calling, document, class-definition, condition, misc].\\n\"\n",
    "        \"Format your output as a list of JSON objects in the following format:\\n\"\n",
    "        \"[\\n\"\n",
    "        \"   {\\n\"\n",
    "        '       \"entity\": \"The Concept\",\\n'\n",
    "        '       \"importance\": \"The contextual importance of the concept on a scale of 1 to 5 (5 being the highest)\",\\n'\n",
    "        '       \"category\": \"The Type of Concept\"\\n'\n",
    "        \"   },\\n\"\n",
    "        \"   {...}\\n\"\n",
    "        \"]\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYS_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    \n",
    "    response = llm.invoke(input=messages).content\n",
    "    print(\"Extract Prompt \", response)\n",
    "    try:\n",
    "        result = json.loads(response)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"\\n\\nWARNING ### Incomplete JSON detected. Attempting to trim...\")\n",
    "        trimmed_response = trim_incomplete_json(response)\n",
    "        print(trimmed_response+\"\\n#####################################################################################################\")\n",
    "        try:\n",
    "            result = json.loads(trimmed_response)\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"\\n\\nERROR ### Failed to parse even after trimming. Here is the buggy response: \", response, \"\\n\\n\")\n",
    "            return None\n",
    "\n",
    "    if result is not None:\n",
    "        result = [dict(item, **metadata) for item in result]\n",
    "\n",
    "    return result\n",
    "\n",
    "def graph_prompt(input_text: str, metadata: dict = {}) -> list:\n",
    "    SYS_PROMPT = (\n",
    "        \"You are a network graph maker who extracts terms and their relations from a given context. \"\n",
    "        \"You are provided with a context chunk (delimited by ```). Your task is to extract the ontology \"\n",
    "        \"of terms mentioned in the given context. These terms should represent the key concepts according to the context.\\n\"\n",
    "        \"Thought 1: While traversing through each sentence, think about whether Data is being passed to it\\n\"\n",
    "        \"\\tTerms may include object creation, entity, class definition, import file, function signature, \\n\"\n",
    "        \"\\tcondition, parameters, documents, service, concept, etc.\\n\"\n",
    "        \"\\tTerms should be as concise as possible but ignore vague definitions\\n\\n\"\n",
    "        \"Thought 2: Think about how these terms can have one-on-one relations with other terms.\\n\"\n",
    "        \"\\tTerms mentioned in the same code or file are typically related to each other.\\n\"\n",
    "        \"\\tTerms can be related to many other terms.\\n\\n\"\n",
    "        \"Thought 3: Determine the relation between each related pair of terms.\\n\\n\"\n",
    "        \"Format your output as a list of JSON objects. Each element of the list contains a pair of terms do not provide an explanation, JUST THE JSON OUTPUT \"\n",
    "        \"and the relationship between them, as follows:\\n\"\n",
    "        \"[\\n\"\n",
    "        \"   {\\n\"\n",
    "        '       \"node_1\": \"A concept from the extracted ontology\",\\n'\n",
    "        '       \"node_2\": \"A related concept from the extracted ontology\",\\n'\n",
    "        '       \"edge\": \"The relationship between node_1 and node_2 in one or two sentences\"\\n'\n",
    "        \"   },\\n\"\n",
    "        \"   {...}\\n\"\n",
    "        \"]\"\n",
    "    )\n",
    "\n",
    "    USER_PROMPT = f\"context: ```{input_text}``` \\n\\n output: \"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYS_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": USER_PROMPT}\n",
    "    ]\n",
    "\n",
    "    response = llm.invoke(input=messages).content\n",
    "    # print(\"Graph Prompt \", response)\n",
    "    try:\n",
    "        result = json.loads(response)\n",
    "    except json.JSONDecodeError:\n",
    "        # print(\"\\n\\nWARNING ### Incomplete JSON detected. Attempting to trim...\")\n",
    "        trimmed_response = trim_incomplete_json(response)\n",
    "        # print(trimmed_response)\n",
    "        # print(\"################################################################################################################\")\n",
    "        try:\n",
    "            result = json.loads(trimmed_response)\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"\\n\\nERROR ### Failed to parse even after trimming. Here is the buggy response: \")\n",
    "            return None\n",
    "\n",
    "    if result is not None:\n",
    "        result = [dict(item, **metadata) for item in result]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe and graph manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def documents2Dataframe(documents) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for chunk in documents:\n",
    "        row = {\n",
    "            \"text\": chunk.page_content,\n",
    "            \"path\": chunk.metadata[\"source\"],\n",
    "            **chunk.metadata,\n",
    "            \"chunk_id\": uuid.uuid4().hex,\n",
    "        }\n",
    "        rows = rows + [row]\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "\n",
    "\n",
    "def df2ConceptsList(dataframe: pd.DataFrame) -> list:\n",
    "    # dataframe.reset_index(inplace=True)\n",
    "    results = dataframe.apply(\n",
    "        lambda row: extract_concepts(\n",
    "            row.text, {\"chunk_id\": row.chunk_id, \"type\": \"concept\"}\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "    # invalid json results in NaN\n",
    "    results = results.dropna()\n",
    "    results = results.reset_index(drop=True)\n",
    "\n",
    "    ## Flatten the list of lists to one single list of entities.\n",
    "    concept_list = np.concatenate(results).ravel().tolist()\n",
    "    return concept_list\n",
    "\n",
    "\n",
    "def concepts2Df(concepts_list) -> pd.DataFrame:\n",
    "    ## Remove all NaN entities\n",
    "    concepts_dataframe = pd.DataFrame(concepts_list).replace(\" \", np.nan)\n",
    "    concepts_dataframe = concepts_dataframe.dropna(subset=[\"entity\"])\n",
    "    concepts_dataframe[\"entity\"] = concepts_dataframe[\"entity\"].apply(\n",
    "        lambda x: x.lower()\n",
    "    )\n",
    "\n",
    "    return concepts_dataframe\n",
    "\n",
    "\n",
    "def df2Graph(dataframe: pd.DataFrame, model=None) -> list:\n",
    "    total_rows = len(dataframe)\n",
    "    processed_rows = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    def process_row(row):\n",
    "        nonlocal processed_rows\n",
    "        result = graph_prompt(row.text, {\"chunk_id\": row.chunk_id,\"path\":row.path})\n",
    "        processed_rows += 1\n",
    "        elapsed_time = time.time() - start_time\n",
    "        avg_time_per_row = elapsed_time / processed_rows\n",
    "        estimated_time_remaining = (total_rows - processed_rows) * avg_time_per_row\n",
    "\n",
    "        print(f\"\\rProcessing: {processed_rows}/{total_rows} rows | \"\n",
    "              f\"Elapsed: {elapsed_time:.2f}s | \"\n",
    "              f\"Estimated time remaining: {estimated_time_remaining:.2f}s\", \n",
    "              end=\"\", flush=True)\n",
    "        return result\n",
    "\n",
    "    results = dataframe.apply(process_row, axis=1)\n",
    "\n",
    "    print(\"\\nProcessing complete!\")\n",
    "    # print(results)\n",
    "\n",
    "    # Filter out None values and flatten the list of lists to one single list of entities.\n",
    "    concept_list = [item for sublist in results if sublist is not None for item in sublist]\n",
    "    return concept_list\n",
    "\n",
    "def graph2Df(nodes_list) -> pd.DataFrame:\n",
    "    ## Remove all NaN entities\n",
    "    graph_dataframe = pd.DataFrame(nodes_list).replace(\" \", np.nan)\n",
    "    graph_dataframe = graph_dataframe.dropna(subset=[\"node_1\", \"node_2\"])\n",
    "    graph_dataframe[\"node_1\"] = graph_dataframe[\"node_1\"].apply(lambda x: x.lower())\n",
    "    graph_dataframe[\"node_2\"] = graph_dataframe[\"node_2\"].apply(lambda x: x.lower())\n",
    "\n",
    "    return graph_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = documents2Dataframe(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To regenerate the graph with LLM, set this to True\n",
    "regenerate = True\n",
    "\n",
    "if regenerate:\n",
    "    concepts_list = df2Graph(df)\n",
    "    dfg1 = graph2Df(concepts_list)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    dfg1.to_csv(output_dir/\"graph.csv\", sep=\"|\", index=False)\n",
    "    df.to_csv(output_dir/\"chunks.csv\", sep=\"|\", index=False)\n",
    "else:\n",
    "    dfg1 = pd.read_csv(output_dir/\"graph.csv\", sep=\"|\")\n",
    "\n",
    "dfg1.replace(\"\", np.nan, inplace=True)\n",
    "dfg1.dropna(subset=[\"node_1\", \"node_2\", 'edge'], inplace=True)\n",
    "dfg1['count'] = 4 \n",
    "## Increasing the weight of the relation to 4. \n",
    "## We will assign the weight of 1 when later the contextual proximity will be calculated.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## connecting node with more contextual proximity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adding count to the edges to design strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contextual_proximity(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Melt the dataframe into a list of nodes\n",
    "    dfg_long = pd.melt(\n",
    "        df, id_vars=[\"chunk_id\", \"path\"], value_vars=[\"node_1\", \"node_2\"], value_name=\"node\"\n",
    "    )\n",
    "    dfg_long.drop(columns=[\"variable\"], inplace=True)\n",
    "\n",
    "    # Self join with chunk id as the key will create a link between terms occurring in the same text chunk.\n",
    "    dfg_wide = pd.merge(dfg_long, dfg_long, on=[\"chunk_id\", \"path\"], suffixes=(\"_1\", \"_2\"))\n",
    "\n",
    "    # drop self loops\n",
    "    self_loops_drop = dfg_wide[dfg_wide[\"node_1\"] == dfg_wide[\"node_2\"]].index\n",
    "    dfg2 = dfg_wide.drop(index=self_loops_drop).reset_index(drop=True)\n",
    "\n",
    "    # Group and count edges.\n",
    "    dfg2 = (\n",
    "        dfg2.groupby([\"node_1\", \"node_2\"])\n",
    "        .agg({\"chunk_id\": [\",\".join, \"count\"], \"path\": \"first\"})\n",
    "        .reset_index()\n",
    "    )\n",
    "    dfg2.columns = [\"node_1\", \"node_2\", \"chunk_id\", \"count\", \"path\"]\n",
    "    dfg2.replace(\"\", np.nan, inplace=True)\n",
    "    dfg2.dropna(subset=[\"node_1\", \"node_2\"], inplace=True)\n",
    "    # Drop edges with 1 count\n",
    "    dfg2 = dfg2[dfg2[\"count\"] != 1]\n",
    "    dfg2[\"edge\"] = \"contextual proximity\"\n",
    "    return dfg2\n",
    "\n",
    "\n",
    "dfg2 = contextual_proximity(dfg1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg = pd.concat([dfg1, dfg2], axis=0)\n",
    "dfg = (\n",
    "    dfg.groupby([\"node_1\", \"node_2\"])\n",
    "    .agg({\n",
    "        \"chunk_id\": \",\".join,\n",
    "        \"edge\": \",\".join,\n",
    "        \"count\": \"sum\",\n",
    "        \"path\": lambda x: \",\".join(set(x))  # Combine unique paths\n",
    "    })\n",
    "    .reset_index()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = pd.concat([dfg['node_1'], dfg['node_2']], axis=0).unique()\n",
    "nodes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drawing the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "G = nx.Graph()\n",
    "\n",
    "## Add nodes to the graph\n",
    "for node in nodes:\n",
    "    G.add_node(\n",
    "        str(node)\n",
    "    )\n",
    "\n",
    "## Add edges to the graph\n",
    "for index, row in dfg.iterrows():\n",
    "    G.add_edge(\n",
    "        str(row[\"node_1\"]),\n",
    "        str(row[\"node_2\"]),\n",
    "        title=row[\"edge\"],\n",
    "        weight=row['count']/4\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "communities_generator = nx.community.girvan_newman(G)\n",
    "top_level_communities = next(communities_generator)\n",
    "next_level_communities = next(communities_generator)\n",
    "communities = sorted(map(sorted, next_level_communities))\n",
    "print(\"Number of Communities = \", len(communities))\n",
    "print(communities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "palette = \"hls\"\n",
    "\n",
    "## Now add these colors to communities and make another dataframe\n",
    "def colors2Community(communities) -> pd.DataFrame:\n",
    "    ## Define a color palette\n",
    "    p = sns.color_palette(palette, len(communities)).as_hex()\n",
    "    random.shuffle(p)\n",
    "    rows = []\n",
    "    group = 0\n",
    "    for community in communities:\n",
    "        color = p.pop()\n",
    "        group += 1\n",
    "        for node in community:\n",
    "            rows += [{\"node\": node, \"color\": color, \"group\": group}]\n",
    "    df_colors = pd.DataFrame(rows)\n",
    "    return df_colors\n",
    "\n",
    "\n",
    "colors = colors2Community(communities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in colors.iterrows():\n",
    "    G.nodes[row['node']]['group'] = row['group']\n",
    "    G.nodes[row['node']]['color'] = row['color']\n",
    "    G.nodes[row['node']]['size'] = G.degree[row['node']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "\n",
    "\n",
    "k_value = 2  # Adjust this value to change node spacing\n",
    "pos = nx.spring_layout(G, k=k_value, iterations=50)\n",
    "\n",
    "\n",
    "net = Network(notebook=False, cdn_resources=\"remote\", height=\"1200px\", width=\"1200px\")\n",
    "\n",
    "\n",
    "for node, (x, y) in pos.items():\n",
    "    net.add_node(\n",
    "        node,\n",
    "        x=x * 200, \n",
    "        y=y * 200,\n",
    "        physics=True,  \n",
    "        **G.nodes[node] \n",
    "    )\n",
    "\n",
    "# Add edges to the Pyvis network\n",
    "for source, target, edge_attrs in G.edges(data=True):\n",
    "    edge_data = edge_attrs.copy()\n",
    "    edge_data['value'] = edge_data['weight']\n",
    "    net.add_edge(\n",
    "        source,\n",
    "        target,\n",
    "        **edge_data\n",
    "    )\n",
    "\n",
    "# Disable physics in Pyvis to maintain the NetworkX layout\n",
    "net.toggle_physics(True)\n",
    "\n",
    "# Save the network\n",
    "net.show(\"./docs/index.html\", notebook=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a Final Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_map = colors.set_index('node')['group'].to_dict()\n",
    "\n",
    "\n",
    "# Create new columns with default values\n",
    "dfg['community_node_1'] = dfg['node_1'].map(community_map)\n",
    "dfg['community_node_2'] = dfg['node_2'].map(community_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_count_node_1 = dfg['community_node_1'].value_counts(dropna=False)\n",
    "\n",
    "# Count the number of nodes in each community for 'community_node_2'\n",
    "community_count_node_2 = dfg['community_node_2'].value_counts(dropna=False)\n",
    "\n",
    "# Output the results\n",
    "# print(\"Community counts for 'community_node_1':\")\n",
    "# print(community_count_node_1)\n",
    "# print(\"\\nCommunity counts for 'community_node_2':\")\n",
    "# print(community_count_node_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg.to_csv('updated_graph.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating an Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GROQ_API_KEY\"]  =\"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "import httpx\n",
    "http_client = httpx.Client(verify=False)\n",
    "llm = ChatGroq(\n",
    "    model=\"llama3-8b-8192\",\n",
    "    temperature=0.1,\n",
    "    http_client=http_client,\n",
    "    max_tokens=1000,\n",
    "    timeout=30,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain.agents.initialize import initialize_agent\n",
    "from langchain.tools import Tool\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "import re\n",
    "import time\n",
    "import logging\n",
    "from tenacity import retry, wait_exponential, stop_after_attempt\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load the CSV file\n",
    "try:\n",
    "    df = pd.read_csv('summary_output_filtered.csv')\n",
    "    df[['start_line', 'end_line']] = df['line_numbers'].str.split('-', expand=True)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading CSV file: {e}\")\n",
    "    df = pd.DataFrame()  # Create an empty DataFrame if file loading fails\n",
    "\n",
    "# Function to get a section of the CSV\n",
    "def get_csv_section(df, start_row, end_row):\n",
    "    try:\n",
    "        df_filtered = df.drop(columns=['chunk_id'])\n",
    "        return df_filtered.iloc[start_row:end_row].to_dict(orient='records')\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting CSV section: {e}\")\n",
    "        return []\n",
    "\n",
    "# Custom tool to read file contents\n",
    "def read_file_contents(file_path, start_line, end_line):\n",
    "    try:\n",
    "        file_path = file_path.strip(\"'\\\"\")\n",
    "        full_path = os.path.join(os.getcwd(), 'TestRepositories', file_path)\n",
    "        full_path = os.path.normpath(full_path)\n",
    "        \n",
    "        if os.path.exists(full_path):\n",
    "            with open(full_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "                logging.info(f\"Reading file: {full_path}\")\n",
    "                lines = file.readlines()\n",
    "                snippet = \"\".join(lines[start_line-1:end_line])\n",
    "                return snippet   # Truncate to 5000 characters\n",
    "        return f\"File not found: {full_path}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error reading file: {e}\"\n",
    "\n",
    "file_reader_tool = Tool(\n",
    "    name=\"FileReader\",\n",
    "    func=read_file_contents,\n",
    "    description=\"Reads the contents of a file given its path\"\n",
    ")\n",
    "\n",
    "def write_markdown_section(identified_nodes, test_scripts, output_file=\"output1.md\", append=False):\n",
    "    mode = \"a\" if append else \"w\"\n",
    "    with open(output_file, mode) as f:\n",
    "        if not append:\n",
    "            f.write(\"# LangChain Test Automation Results\\n\\n\")\n",
    "        \n",
    "        f.write(\"## Identified Chunks\\n\\n\")\n",
    "        for node in identified_nodes:\n",
    "            f.write(f\"- Coverage_file 1: {node['node_1']}\\n\")\n",
    "            f.write(f\"  Coverage_file 2: {node['node_2']}\\n\")\n",
    "            f.write(f\"  Task: {node['edge']}\\n\")\n",
    "        \n",
    "        f.write(\"## Generated Test Scripts\\n\\n\")\n",
    "        for i, script in enumerate(test_scripts, 1):\n",
    "            f.write(f\"### Script {i}\\n\\n\")\n",
    "            f.write(\"```python\\n\")\n",
    "            f.write(script)\n",
    "            f.write(\"\\n```\\n\\n\")\n",
    "            f.write(\"---\\n\\n\")\n",
    "\n",
    "# Custom tool to generate test automation scripts\n",
    "@retry(wait=wait_exponential(multiplier=1, min=4, max=10), stop=stop_after_attempt(3))\n",
    "def generate_test_script(inputs):\n",
    "    try:\n",
    "        dictionary = ast.literal_eval(inputs)\n",
    "        node_1 = dictionary['node_1']\n",
    "        node_2 = dictionary['node_2']\n",
    "        edge = dictionary['edge']\n",
    "        path = dictionary['path']\n",
    "        file_contents = dictionary.get('file_contents', 'File contents not provided')[:2000]  # Truncate to 2000 characters\n",
    "\n",
    "        template = \"\"\"\n",
    "        Create a Python test automation script for the following scenario:\n",
    "        - Node 1: {node_1}\n",
    "        - Node 2: {node_2}\n",
    "        - Edge: {edge}\n",
    "        - Path: {path}\n",
    "        - File Contents: {file_contents}\n",
    "\n",
    "        Note: contextual proximity implies that while creating the graph they appeared in the same chunk of text.\n",
    "        The script should:\n",
    "        1. Set up the test environment.\n",
    "        2. Read the file contents from the given path if you need more information.\n",
    "        3. Provide detailed test cases for testing the file and its impact on a system level.\n",
    "        4. Write comprehensive tests for the edge with respect to the file contents or assume data if necessary.\n",
    "        5. Clean up the test environment.\n",
    "\n",
    "        Please provide the complete Python script (assume data if necessary).\n",
    "        \"\"\"\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"node_1\", \"node_2\", \"edge\", \"path\", 'file_contents'],\n",
    "            template=template\n",
    "        )\n",
    "        return llm.invoke(prompt.format(node_1=node_1, node_2=node_2, edge=edge, path=path, file_contents=file_contents))[:2000]  # Truncate to 2000 characters\n",
    "    except Exception as e:\n",
    "        return f\"Error generating test script: {e}\"\n",
    "\n",
    "test_generator_tool = Tool(\n",
    "    name=\"TestGenerator\",\n",
    "    func=generate_test_script,\n",
    "    description=\"Generates a test automation script given inputs in dictionary form with the keys 'node_1', 'node_2', 'edge', and 'path'\"\n",
    ")\n",
    "\n",
    "# Create the main agent\n",
    "tools = [file_reader_tool, test_generator_tool]\n",
    "\n",
    "try:\n",
    "    agent = initialize_agent(\n",
    "        tools,\n",
    "        llm,\n",
    "        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "        verbose=False,\n",
    "        handle_parsing_errors=True\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing agent: {e}\")\n",
    "    agent = None\n",
    "\n",
    "# Function to extract list of dictionaries from LLM output\n",
    "def extract_dicts_from_llm_output(output):\n",
    "    try:\n",
    "        dict_pattern = r'\\{[^{}]*\\}'\n",
    "        dict_strings = re.findall(dict_pattern, output)\n",
    "        \n",
    "        dicts = []\n",
    "        for d_str in dict_strings:\n",
    "            try:\n",
    "                d = ast.literal_eval(d_str)\n",
    "                if isinstance(d, dict) and all(key in d for key in ['node_1', 'node_2', 'edge', 'priority', 'path']):\n",
    "                    dicts.append(d)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return dicts\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting dictionaries from LLM output: {e}\")\n",
    "        return []\n",
    "\n",
    "# Process the CSV in sections\n",
    "section_size = 5  # Reduced from 10 to 5\n",
    "total_rows = len(df)\n",
    "test_scripts = []\n",
    "identified_nodes = []\n",
    "\n",
    "start_time = time.time()\n",
    "total_sections = (total_rows + section_size - 1) // section_size\n",
    "\n",
    "for section, start_row in enumerate(range(0, total_rows, section_size), 1):\n",
    "    section_start_time = time.time()\n",
    "    try:\n",
    "        end_row = min(start_row + section_size, total_rows)\n",
    "        csv_section = get_csv_section(df, start_row, end_row)\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        estimated_total_time = (elapsed_time / section) * total_sections\n",
    "        estimated_time_left = estimated_total_time - elapsed_time\n",
    "\n",
    "        print(f\"\\nProcessing rows {start_row} to {end_row - 1} (Section {section}/{total_sections}):\")\n",
    "        print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "        print(f\"Estimated time left: {estimated_time_left:.2f} seconds\")\n",
    "\n",
    "        llm_context = f\"\"\"\n",
    "        Here is a section of the CSV file (rows {start_row} to {end_row - 1}):\n",
    "\n",
    "        {csv_section}\n",
    "\n",
    "        Select which values of nodes, edges, paths are most relevant for SYSTEM LEVEL test automation scripts. Do not repeat nodes and edges.\n",
    "        Assign them high or low priority for testing.\n",
    "        Write them in dictionary format with keys: 'node_1','node_2','edge','priority','path'.\n",
    "        Provide your selections as a list of dictionaries.\n",
    "        \"\"\"[:2000]  # Truncate to 2000 characters\n",
    "\n",
    "        llm_plan = llm.invoke(llm_context)\n",
    "        time.sleep(10)  # Increased delay between API calls\n",
    "\n",
    "        plan_list = extract_dicts_from_llm_output(llm_plan.content)\n",
    "\n",
    "        for item in plan_list:\n",
    "            node_1 = item['node_1']\n",
    "            node_2 = item['node_2']\n",
    "            edge = item['edge']\n",
    "            path = item['path']\n",
    "            priority = item['priority']\n",
    "            \n",
    "            identified_nodes.append({\n",
    "                'node_1': node_1,\n",
    "                'node_2': node_2,\n",
    "                'edge': edge,\n",
    "                'priority': priority\n",
    "            })\n",
    "            \n",
    "            script_input = str({\n",
    "                'node_1': node_1,\n",
    "                'node_2': node_2,\n",
    "                'edge': edge,\n",
    "                'path': path\n",
    "            })[:2000]  # Truncate to 2000 characters\n",
    "            if agent:\n",
    "                result = agent.run(f\"Use the TestGenerator tool to create a test script for: {script_input}\")\n",
    "                print(\"The Script input for the following nodes are : \", script_input)\n",
    "                print(\"-----------------------------------------------------------------------------------------------------------------\")\n",
    "                print(\"The result for the following nodes are : \", result)\n",
    "                time.sleep(10)  # Increased delay between API calls\n",
    "                test_scripts.append(result[:2000])  # Truncate to 2000 characters\n",
    "            else:\n",
    "                print(\"Agent not initialized. Skipping test script generation.\")\n",
    "\n",
    "        write_markdown_section(identified_nodes, test_scripts, append=(section > 1))\n",
    "\n",
    "        identified_nodes = []\n",
    "        test_scripts = []\n",
    "        print(identified_nodes, test_scripts)\n",
    "        print(\"##############################################################################################################\")\n",
    "        section_time = time.time() - section_start_time\n",
    "        print(f\"Time taken for this section: {section_time:.2f} seconds\")\n",
    "        time.sleep(10)  # Increased delay between sections\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing rows {start_row} to {end_row - 1}: {e}\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTotal processing time: {total_time:.2f} seconds\")\n",
    "print(\"Results have been written to output.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
