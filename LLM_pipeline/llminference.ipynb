{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import httpx\n",
    "import os\n",
    "\n",
    "# As mentioned in the above Note try to setup Dell certificate in your environment to avoid verify=False(SSL verification is disabled).\n",
    "http_client=httpx.Client(verify=False)\n",
    "client = OpenAI(\n",
    "    base_url='https://opensource-challenger-api.prdlvgpu1.aiaccel.dell.com/v1',\n",
    "    http_client=http_client,\n",
    "    api_key=os.environ['LLM_API']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56548"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../output1.txt', 'r') as f:\n",
    "    required_prompt = f.read()\n",
    "len(required_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: mixtral-8x7b-instruct-v01\n",
      "\n",
      "\n",
      "Hi bob, I am a computer algorithm designed to interact with humans. I don't have personal experiences or feelings, but I am programmed to understand and respond to human language in a way that simulates conversation. How can I help you today? Is there something specific you would like to talk about?"
     ]
    }
   ],
   "source": [
    "streaming = True\n",
    "max_output_tokens = 100\n",
    "\n",
    "# Read the content of 'output.txt' as the required prompt\n",
    "# with open('../output.txt', 'r') as f:\n",
    "#     required_prompt = f.read()\n",
    "\n",
    "# Available Models list\n",
    "available_models = [\"mixtral-8x7b-instruct-v01\", \"llamaguard-7b\", \"gemma-7b-it\", \"mistral-7b-instruct-v02\", \"phi-2\", \"llama-2-70b-chat\", \"phi-3-mini-128k-instruct\", \"llama-3-8b-instruct\"]\n",
    "\n",
    "# Let's select the model from the available list\n",
    "model_selected = available_models[0]\n",
    "\n",
    "\n",
    "print(f\"Model: {model_selected}\")\n",
    "completion = client.completions.create(\n",
    "    model=model_selected,\n",
    "    max_tokens=max_output_tokens,\n",
    "    prompt=\"Hi, I am bob, tell me about yourself.\",\n",
    "    stream=streaming\n",
    ")\n",
    "\n",
    "if streaming:\n",
    "    for chunk in completion:\n",
    "        print(chunk.choices[0].text, end='')\n",
    "else:\n",
    "    print(completion.choices[0].text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 7f4a3b7a-c43e-4d4d-8d02-32a536c98eae not found for run b5e7127d-d482-4809-af11-4fa954e8ebde. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\" Hello Bob! It's nice to meet you. How can I help you today? Is there something you would like to know or talk about? I'm here to answer any questions you have to the best of my ability.\", response_metadata={'token_usage': {'completion_tokens': 48, 'prompt_tokens': 14, 'total_tokens': 62}, 'model_name': 'mixtral-8x7b-instruct-v01', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-b5e7127d-d482-4809-af11-4fa954e8ebde-0', usage_metadata={'input_tokens': 14, 'output_tokens': 48, 'total_tokens': 62})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "store = {}  # memory is maintained outside the chain\n",
    "\n",
    "def get_session_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    base_url='https://opensource-challenger-api.prdlvgpu1.aiaccel.dell.com/v1',\n",
    "    model=model_selected,\n",
    "    openai_api_key=os.environ['LLM_API'],\n",
    "    http_client=http_client\n",
    ")\n",
    "\n",
    "chain = RunnableWithMessageHistory(llm, get_session_history)\n",
    "chain.invoke(\n",
    "    \"Hi I'm Bob.\",\n",
    "    config={\"configurable\": {\"session_id\": \"1\"}},\n",
    ")  # session_id determines thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 01dee4e9-428e-4a98-9585-bec7cf7ef13b not found for run fb4faafb-4435-4391-aa85-d8c13d9b8e3b. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\" Hello Bob! It's nice to meet you. Is there something specific you would like to talk about or ask me? I'm here to help with any questions you have to the best of my ability.\", response_metadata={'token_usage': {'completion_tokens': 44, 'prompt_tokens': 15, 'total_tokens': 59}, 'model_name': 'mixtral-8x7b-instruct-v01', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-fb4faafb-4435-4391-aa85-d8c13d9b8e3b-0', usage_metadata={'input_tokens': 15, 'output_tokens': 44, 'total_tokens': 59})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "store = {}  # memory is maintained outside the chain\n",
    "\n",
    "def get_session_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "        return store[session_id]\n",
    "\n",
    "    memory = ConversationBufferWindowMemory(\n",
    "        chat_memory=store[session_id],\n",
    "        k=3,\n",
    "        return_messages=True,\n",
    "    )\n",
    "    assert len(memory.memory_variables) == 1\n",
    "    key = memory.memory_variables[0]\n",
    "    messages = memory.load_memory_variables({})[key]\n",
    "    store[session_id] = InMemoryChatMessageHistory(messages=messages)\n",
    "    return store[session_id]\n",
    "\n",
    "llm = ChatOpenAI(base_url='https://opensource-challenger-api.prdlvgpu1.aiaccel.dell.com/v1',\n",
    "    model=model_selected,\n",
    "    openai_api_key=os.environ['LLM_API'],\n",
    "    http_client=http_client)\n",
    "\n",
    "chain = RunnableWithMessageHistory(llm, get_session_history)\n",
    "chain.invoke(\n",
    "    \"Hi I'm Bob.\",\n",
    "    config={\"configurable\": {\"session_id\": \"1\"}},\n",
    ")  # session_id determines thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: mixtral-8x7b-instruct-v01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run c7cca3da-4cb6-472b-9339-f6b837bd0f2d not found for run 8ce21e30-5c35-4f4e-8950-b32f3cddf423. Treating as a root run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chat History for session 1:\n",
      "\n",
      "First interaction:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 5207cc8d-87b3-4ca8-aa61-2aff9e8de81c not found for run 64872ab4-ee3d-409e-8be6-b81d69112391. Treating as a root run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant:  Hello Bob, I'm an assistant. I don't have a personal name as I'm here to help you with any questions or information you need. How can I assist you today?\n",
      "\n",
      "Chat History for session 1:\n",
      "human: Hi, I'm Bob. What's your name?\n",
      "ai:  Hello Bob, I'm an assistant. I don't have a personal name as I'm here to help you with any questions or information you need. How can I assist you today?\n",
      "\n",
      "Second interaction:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run a537d0d0-8aa3-4b3b-b6e7-c0355b3f4b62 not found for run f6216de6-c11b-42bb-8a9b-026fca68710d. Treating as a root run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant:  Hello Bob, nice to meet you too! I can tell you that Python is a high-level, interpreted programming language that was created by Guido van Rossum and first released in 1991. It is designed with an emphasis on code readability, and its syntax allows programmers to express concepts in fewer lines of code than would be possible in languages such as C++ or Java.\n",
      "\n",
      "Python supports multiple programming paradigms, including procedural, object-oriented, and functional programming. It has a large standard library that is available for use without additional installation, which includes modules for working with files, interacting with operating systems, and handling common data structures.\n",
      "\n",
      "Python is widely used in various domains, such as web development, scientific computing, data analysis, artificial intelligence, machine learning, and automation. It also has a large and active community that contributes to its development and provides support to users.\n",
      "\n",
      "Chat History for session 1:\n",
      "human: Hi, I'm Bob. What's your name?\n",
      "ai:  Hello Bob, I'm an assistant. I don't have a personal name as I'm here to help you with any questions or information you need. How can I assist you today?\n",
      "human: Nice to meet you! What do you know about Python?\n",
      "ai:  Hello Bob, nice to meet you too! I can tell you that Python is a high-level, interpreted programming language that was created by Guido van Rossum and first released in 1991. It is designed with an emphasis on code readability, and its syntax allows programmers to express concepts in fewer lines of code than would be possible in languages such as C++ or Java.\n",
      "\n",
      "Python supports multiple programming paradigms, including procedural, object-oriented, and functional programming. It has a large standard library that is available for use without additional installation, which includes modules for working with files, interacting with operating systems, and handling common data structures.\n",
      "\n",
      "Python is widely used in various domains, such as web development, scientific computing, data analysis, artificial intelligence, machine learning, and automation. It also has a large and active community that contributes to its development and provides support to users.\n",
      "\n",
      "Third interaction (different session):\n",
      "Assistant:  Hello Alice, I'd be happy to help! However, I'll need a bit more information to give you an accurate weather report. Could you please tell me your location? In general, I can tell you that weather conditions vary greatly depending on the time of year and the specific area of the world. Some places might be experiencing sunny skies and warm temperatures, while others could be dealing with cold, snowy conditions or rainy weather. Let me know if you need a report for a specific location!\n",
      "\n",
      "Chat History for session 1:\n",
      "human: Hi, I'm Bob. What's your name?\n",
      "ai:  Hello Bob, I'm an assistant. I don't have a personal name as I'm here to help you with any questions or information you need. How can I assist you today?\n",
      "human: Nice to meet you! What do you know about Python?\n",
      "ai:  Hello Bob, nice to meet you too! I can tell you that Python is a high-level, interpreted programming language that was created by Guido van Rossum and first released in 1991. It is designed with an emphasis on code readability, and its syntax allows programmers to express concepts in fewer lines of code than would be possible in languages such as C++ or Java.\n",
      "\n",
      "Python supports multiple programming paradigms, including procedural, object-oriented, and functional programming. It has a large standard library that is available for use without additional installation, which includes modules for working with files, interacting with operating systems, and handling common data structures.\n",
      "\n",
      "Python is widely used in various domains, such as web development, scientific computing, data analysis, artificial intelligence, machine learning, and automation. It also has a large and active community that contributes to its development and provides support to users.\n",
      "\n",
      "Chat History for session 2:\n",
      "human: Hello, I'm Alice. Can you tell me about the weather?\n",
      "ai:  Hello Alice, I'd be happy to help! However, I'll need a bit more information to give you an accurate weather report. Could you please tell me your location? In general, I can tell you that weather conditions vary greatly depending on the time of year and the specific area of the world. Some places might be experiencing sunny skies and warm temperatures, while others could be dealing with cold, snowy conditions or rainy weather. Let me know if you need a report for a specific location!\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model: {model_selected}\")\n",
    "\n",
    "# Chat history management\n",
    "store = {}  # memory is maintained outside the chain\n",
    "\n",
    "def get_session_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "def print_chat_history(session_id: str):\n",
    "    history = get_session_history(session_id)\n",
    "    print(f\"\\nChat History for session {session_id}:\")\n",
    "    for message in history.messages:\n",
    "        print(f\"{message.type}: {message.content}\")\n",
    "\n",
    "# Create a proper LLM object\n",
    "llm = ChatOpenAI(\n",
    "    base_url='https://opensource-challenger-api.prdlvgpu1.aiaccel.dell.com/v1',\n",
    "    model=model_selected,\n",
    "    openai_api_key=os.environ['LLM_API'],\n",
    "    http_client=http_client\n",
    ")\n",
    "\n",
    "chain = RunnableWithMessageHistory(llm, get_session_history)\n",
    "\n",
    "# Check initial state\n",
    "print_chat_history(\"1\")\n",
    "\n",
    "# First interaction\n",
    "print(\"\\nFirst interaction:\")\n",
    "response1 = chain.invoke(\n",
    "    \"Hi, I'm Bob. What's your name?\",\n",
    "    config={\"configurable\": {\"session_id\": \"1\"}},\n",
    ")\n",
    "print(\"Assistant:\", response1.content)\n",
    "\n",
    "# Check history after first interaction\n",
    "print_chat_history(\"1\")\n",
    "\n",
    "# Second interaction\n",
    "print(\"\\nSecond interaction:\")\n",
    "response2 = chain.invoke(\n",
    "    \"Nice to meet you! What do you know about Python?\",\n",
    "    config={\"configurable\": {\"session_id\": \"1\"}},\n",
    ")\n",
    "print(\"Assistant:\", response2.content)\n",
    "\n",
    "# Check history after second interaction\n",
    "print_chat_history(\"1\")\n",
    "\n",
    "# Third interaction with a different session\n",
    "print(\"\\nThird interaction (different session):\")\n",
    "response3 = chain.invoke(\n",
    "    \"Hello, I'm Alice. Can you tell me about the weather?\",\n",
    "    config={\"configurable\": {\"session_id\": \"2\"}},\n",
    ")\n",
    "print(\"Assistant:\", response3.content)\n",
    "\n",
    "# Check history for both sessions\n",
    "print_chat_history(\"1\")\n",
    "print_chat_history(\"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
